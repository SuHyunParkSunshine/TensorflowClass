{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVE-EXIwz8bl"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iyWPGkZmMAJn"
      },
      "source": [
        "##"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_BiBqFm52drj"
      },
      "source": [
        "# 2.7 linear algebra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r7f-gFB40DX"
      },
      "outputs": [],
      "source": [
        "# https://www.tensorflow.org/api_docs/python/tf/linalg\n",
        "# Operations for linear algebra.\n",
        "\n",
        "import tensorflow as tf\n",
        "#1 L2 norm (유클리디언 거리)\n",
        "# https://www.tensorflow.org/api_docs/python/tf/norm\n",
        "\n",
        "a = tf.constant([1, 2], dtype=tf.float32)\n",
        "print('a', a)\n",
        "print('tf.norm(a)', tf.norm(a)) # tf.linalg.norm(a)\n",
        "\n",
        "#2 transpose\n",
        "A = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
        "print('A', A)\n",
        "print('tf.linalg.matrix_transpose(A)', tf.linalg.matrix_transpose(A))\n",
        "\n",
        "#3 determinant\n",
        "print('tf.linalg.det(A)', tf.linalg.det(A))\n",
        "\n",
        "#4 inverse matrix\n",
        "B =  tf.linalg.inv(A)\n",
        "print('B', B)\n",
        "\n",
        "#5 matrix multiplication\n",
        "print('tf.matmul(A, B)', tf.matmul(A, B))  # tf.linalg.matmul(A, B)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rDWfJGOn5zUP"
      },
      "source": [
        "**matrix equation**\n",
        "\n",
        "Ax = b\n",
        "\n",
        "==> x = A<sup>-1</sup> x b\n",
        "\n",
        "A<sup>-1</sup> = $\\frac{1}{DET(A)}$ A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SLoqrqqM6wJ_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(-11.000001, shape=(), dtype=float32)\n",
            "x1 tf.Tensor(\n",
            "[[2.]\n",
            " [1.]], shape=(2, 1), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[2.]\n",
            " [1.]], shape=(2, 1), dtype=float32) tf.Tensor(\n",
            "[[2.]\n",
            " [1.]], shape=(2, 1), dtype=float32)\n",
            "(<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
            "array([[2.],\n",
            "       [1.]], dtype=float32)>, <tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
            "array([[2.],\n",
            "       [1.]], dtype=float32)>)\n"
          ]
        }
      ],
      "source": [
        "#1 Ax = b\n",
        "A = tf.constant([[1, 2],\n",
        "                 [3, -5]], dtype=tf.float32)\n",
        "\n",
        "b = tf.constant([[4],\n",
        "                 [1]], dtype=tf.float32)\n",
        "\n",
        "#2 DET(A)\n",
        "print(tf.linalg.det(A))\n",
        "\n",
        "def solve(A, b):\n",
        "  # x = A^-1 x b\n",
        "  x1 = tf.matmul(tf.linalg.inv(A), b)\n",
        "  print('x1', x1)\n",
        "\n",
        "  # https://www.tensorflow.org/api_docs/python/tf/linalg/solve\n",
        "  # Solves systems of linear equations.\n",
        "  x2 = tf.linalg.solve(A, b)  # x = 2, y = 1\n",
        "  print(x1, x2)\n",
        "\n",
        "  return x1, x2\n",
        "\n",
        "print(solve(A, b))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "l9xUGEXn8puD"
      },
      "source": [
        "**LU decomposition**\n",
        "\n",
        "A = LU 로 분해하여\n",
        "\n",
        "Ax = b 를 푸는 과정에 활용해 보면,\n",
        "\n",
        "=> LUx = b\n",
        "\n",
        "=> Ux = Z  로 놓고\n",
        "\n",
        "=> LZ = b  를 풀고\n",
        "\n",
        "=> Ux = z 를 풀면\n",
        "\n",
        "x 를 구할 수 있음\n",
        "(역행렬 방법과 비교하여 빠르고, 메모리 적게 사용해서 계산 가능)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pi3HiAi59fXt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.linalg.lu(A) tf.Tensor(\n",
            "[[ 3.         -5.        ]\n",
            " [ 0.33333334  3.6666667 ]], shape=(2, 2), dtype=float32) tf.Tensor([1 0], shape=(2,), dtype=int32)\n",
            "x tf.Tensor(\n",
            "[[2.]\n",
            " [1.]], shape=(2, 1), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "A = tf.constant([[1, 2],\n",
        "                 [3, -5]], dtype=tf.float32)\n",
        "\n",
        "b = tf.constant([[4],\n",
        "                 [1]], dtype=tf.float32)\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/linalg/lu\n",
        "# Computes the LU decomposition of one or more square matrices.\n",
        "\n",
        "lu, p = tf.linalg.lu(A)\n",
        "print('tf.linalg.lu(A)', lu, p)\n",
        "\n",
        "x = tf.linalg.lu_solve(lu, p, b)\n",
        "print('x', x)\n",
        "\n",
        "# L = tf.linalg.band_part(lu,-1,0) - tf.linalg.diag(tf.linalg.diag_part(lu)) + tf.linalg.diag(tf.ones(shape = lu.shape[0],))\n",
        "# U = tf.linalg.band_part(lu, 0, -1)\n",
        "# permu_operator = tf.linalg.LinearOperatorPermutation(p)\n",
        "# P = permu_operator.to_dense()\n",
        "\n",
        "# print('L', L)\n",
        "# print('U', U)\n",
        "\n",
        "# AA = tf.matmul(P, tf.matmul(L,U), transpose_a = True)\n",
        "# print('AA', AA)\n",
        "\n",
        "# AAA = tf.linalg.lu_reconstruct(lu, p)\n",
        "# print('AAA', AAA)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4SNn22puAZmm"
      },
      "source": [
        "**least squares**\n",
        "\n",
        "(0, 6), (1, 0), (2, 0) 을 지나는 가장 가까운 직선의 방정식은?\n",
        "\n",
        "6 = a x 0 + b\n",
        "\n",
        "0 = a x 1 + b\n",
        "\n",
        "0 = a x 2 + b\n",
        "\n",
        "위 수식을 모두 만족하는 a 는 존재하는가?\n",
        "\n",
        "=> 존재하지 않으면 **차이가 최소(에러가 최소)**가 되는 근사값 $\\hat{a}$ 을 구해보자!\n",
        "\n",
        "Error<sup>2</sup> = (6 - (a x 0 + b))<sup>2</sup> + (0 - (a x 1 + b))<sup>2</sup> + (0 - (a x 2 + b))<sup>2</sup>\n",
        "\n",
        "=> 행렬로 풀려면 공식 이용\n",
        "\n",
        "$X = \\begin{bmatrix}\\hat{a}\\\\\n",
        "\\hat{b}\n",
        "\\end{bmatrix}$\n",
        "\n",
        "(A<sup>T</sup>A)X = A<sup>T</sup>b\n",
        "\n",
        "(참고: https://en.wikipedia.org/wiki/Linear_least_squares)\n",
        "\n",
        "\n",
        "=> 2차 방정식이므로 $\\hat{a}$ 과 $\\hat{b}$ 에 대해 미분하자!\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tQYksr0Z_N8a"
      },
      "source": [
        "# 2.8 gradient decent\n",
        "\n",
        "수학에서 경사 하강법은 미분 가능 함수의 로컬 최소값을 찾기 위한 1차 반복 최적화 알고리즘입니다.\n",
        "\n",
        "알고리즘: 현재 지점에서 함수의 기울기의 반대 방향(빼줌)으로 반복되는 단계를 수행하는 것이며, 그 결과로 local minimum을 찾을 수 있습니다.\n",
        "\n",
        "$x = x - lr * dx$\n",
        "\n",
        "\n",
        "[gradient decent](https://en.wikipedia.org/wiki/Gradient_descent)\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1H77kMZ1UhsB3N7HRXJ77StzJzTyXodf0\" width=\"400\"/>\n",
        "\n",
        "[Animation](https://medium.com/onfido-tech/machine-learning-101-be2e0a86c96a)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mAdWjUPAA7sH"
      },
      "source": [
        "\n",
        "예)\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1vcxLliVPoM6Lw6HfFEQAFiXDf3J0CjL_\" width=\"200\"/>\n",
        "\n",
        "\n",
        "$ F = (x-1)^2$\n",
        "\n",
        "=> $x^2 -2x + 1$\n",
        "\n",
        "[2차 함수](https://www.geogebra.org/m/xgdQkfk3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "w7EJ3u3DETMt",
        "outputId": "fe4a8aff-0b10-4bf9-9fc5-2972040fdfe0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, '$y=x^2-2x+1$')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAc4AAAHACAYAAAAr7IjAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAABGlUlEQVR4nO3deVyU1eIG8OdlGxaHUUQYEERUEBFREffdXEAvudQtWyxv5c3Uymy1btlOy23xl2l1u9dbmWlWmt1MpRTcFVTcd0WQRQR0hnWAmff3BzKJAjowzJnl+X4+8/nEzDA8l1s+nvOe9xxJlmUZREREdEucRAcgIiKyJSxOIiIiE7A4iYiITMDiJCIiMgGLk4iIyAQsTiIiIhOwOImIiEzA4iQiIjKBi+gAohkMBuTk5ECpVEKSJNFxiIhIAFmWUVxcjMDAQDg5NT6mdPjizMnJQXBwsOgYRERkBbKyshAUFNToexy+OJVKJYCaX5a3t7fgNEREJIJWq0VwcLCxExrj8MVZOz3r7e3N4iQicnC3csmOi4OIiIhMwOIkIiIyAYuTiIjIBCxOIiIiE7A4iYiITMDiJCIiMgGLk4iIyAQsTiIiIhOwOImIiEzA4iQiIjIBi5OIiMgELE4iIiITsDjNRG+QRUcgIiILYHE2k8Eg46mV6Yh5Iwk5V8pFxyEiohbG4mwmJycJWUVl0JRX4Y9jF0XHISKiFsbiNIPRkf4AgKRj+YKTEBFRS2NxmsHobjXFufNMAYorqgSnISKilsTiNIPO7bwQ6uuFKr2MracKRMchIqIWxOI0A0mSMLqbHwDg96O8zklEZM9YnGZSO1276UQ+qvUGwWmIiKilWG1xLlmyBNHR0fD29oa3tzcGDhyI3377zfj69OnTIUlSnceAAQOE5e0T0gatPV1xpawKe89fFpaDiIhaltUWZ1BQEN555x2kpaUhLS0No0aNwsSJE3HkyBHje+Li4pCbm2t8rFu3TlheF2cnjOp6dbqWt6UQEdktqy3OhIQEjB8/HuHh4QgPD8dbb72FVq1aYdeuXcb3KBQKqNVq48PHx0dg4mtuSzl6EbLMnYSIiOyR1RbntfR6PVasWIHS0lIMHDjQ+HxycjL8/PwQHh6OGTNmID//5vdR6nQ6aLXaOg9zGRbeDm7OTsgoLMOZS6Vm+1wiIrIeVl2chw4dQqtWraBQKDBz5kysXr0akZGRAID4+Hh8++232LRpEz744AOkpqZi1KhR0Ol0jX5mYmIiVCqV8REcHGy2vK0ULhjQuS0ATtcSEdkrSbbiOcXKykpkZmbiypUr+PHHH/Hll18iJSXFWJ7Xys3NRUhICFasWIEpU6Y0+Jk6na5OuWq1WgQHB0Oj0cDb27vZmb/ZmYGXfz6CPiFt8ONjg5r9eURE1PK0Wi1UKtUtdYFVjzjd3NzQpUsXxMbGIjExET179sTChQvrfW9AQABCQkJw6tSpRj9ToVAYV+rWPszptqu3pezLvIyCksZHv0REZHusujivJ8tyg1OxhYWFyMrKQkBAgIVT1RXY2gNR7b0hy8Am7l1LRGR3rLY4X3zxRWzduhUZGRk4dOgQXnrpJSQnJ+O+++5DSUkJnnnmGezcuRMZGRlITk5GQkICfH19MXnyZNHRMaabGgCw8Wie4CRERGRuLqIDNOTixYuYNm0acnNzoVKpEB0djfXr12PMmDEoLy/HoUOH8PXXX+PKlSsICAjAyJEjsXLlSiiVStHRMba7Pz76/SS2nipAWWU1PN2s9tdMREQmsto/0f/97383+JqHhwc2bNhgwTSmiVArEdTGAxcul2PrqQKM664WHYmIiMzEaqdqbZkkSRhzzWYIRERkP1icLaS2OP84dpGbvhMR2REWZwvp19EHKg9XXOam70REdoXF2UJcnJ1wW0TNpu+criUish8szhZkvM55jJu+ExHZCxZnCxoW3g5uLk44X1iGkxdLRMchIiIzYHG2IC+FC4Z08QUAbDzCzRCIiOwBi7OFjb06XbuR1zmJiOwCi7OFjY70hyQBh7I1yL5SLjoOERE1E4uzhfm2UiA2pA0ATtcSEdkDFqcF1G65t4HFSURk81icFjA2sqY495wrwuXSSsFpiIioOVicFtChrSci1EoYZOD3Y1wkRERky1icFvLndC2Lk4jIlrE4LaS2OLeeuoSyymrBaYiIqKlYnBbSLaDmjE5dtQFbTl4SHYeIiJqIxWkhkiRxupaIyA6wOC2otjj/OHYRVTyjk4jIJrE4LahPSBv4tnKDtqIaO88Uio5DRERNwOK0IGcnCWOu3tO5npshEBHZJBanhcVF1RTnxiMXoTfwjE4iIlvD4rSwgZ3aQunugoISHfZlXhYdh4iITMTitDA3FyeM7lZz1Nj6w5yuJSKyNSxOAWqna9cfzoMsc7qWiMiWsDgFGBbWDh6uzsi+Uo4jOVrRcYiIyAQsTgE83Jwxoms7AJyuJSKyNSxOQYzTtbwthYjIprA4BRkZ4QdXZwmn80twOr9YdBwiIrpFLE5BvN1dMaSLLwBO1xIR2RIWp0C107W/sTiJiGwGi1OgMZFqODtJOJKjxfnCUtFxiIjoFrA4BfLxcsOATj4AOOokIrIVLE7B4qMCALA4iYhsBYtTsHHd1ZAk4EDWFVy4XCY6DhER3QSLU7B2SgX6dqyZruXqWiIi68fitALjr9m7loiIrBuL0wrEXb3OmXb+MvI0FYLTEBFRY6y2OJcsWYLo6Gh4e3vD29sbAwcOxG+//WZ8XZZlvPrqqwgMDISHhwdGjBiBI0eOCEzcdGqVO2I6tAYAbOAWfEREVs1qizMoKAjvvPMO0tLSkJaWhlGjRmHixInGcnzvvffw4YcfYtGiRUhNTYVarcaYMWNQXGyb29eN71G7ujZXcBIiImqMJNvQgZA+Pj54//338dBDDyEwMBBz587F888/DwDQ6XTw9/fHu+++i0cfffSWP1Or1UKlUkGj0cDb27ulot/UhctlGPLuZjhJwO4XR6OdUiEsCxGRozGlC6x2xHktvV6PFStWoLS0FAMHDsS5c+eQl5eHsWPHGt+jUCgwfPhw7NixQ2DSpgtq44noIBUMMk9MISKyZlZdnIcOHUKrVq2gUCgwc+ZMrF69GpGRkcjLqykWf3//Ou/39/c3vtYQnU4HrVZb52EtJlydrl13kNO1RETWyqqLs2vXrkhPT8euXbvw2GOP4cEHH8TRo0eNr0uSVOf9sizf8Nz1EhMToVKpjI/g4OAWyd4Utdc5d58rREGJTnAaIiKqj1UXp5ubG7p06YLY2FgkJiaiZ8+eWLhwIdTqmvserx9d5ufn3zAKvd78+fOh0WiMj6ysrBbLb6pgn2uma3lPJxGRVbLq4ryeLMvQ6XQIDQ2FWq1GUlKS8bXKykqkpKRg0KBBjX6GQqEw3uJS+7AmtaPOdYc4XUtEZI1cRAdoyIsvvoj4+HgEBwejuLgYK1asQHJyMtavXw9JkjB37ly8/fbbCAsLQ1hYGN5++214enri3nvvFR29WSb0CMA7vx3HrrM107W+rbi6lojImlhtcV68eBHTpk1Dbm4uVCoVoqOjsX79eowZMwYA8Nxzz6G8vByzZs3C5cuX0b9/f2zcuBFKpVJw8uapna49eEGD9YfzcP+AENGRiIjoGjZ1H2dLsJb7OK/1WcoZvPPbcQzq3BbLZwwQHYeIyO7Z3X2cjqb2tpTa6VoiIrIeLE4rdO3qWu5dS0RkXVicVqp2de2v3AyBiMiqsDit1LXTtZeKOV1LRGQtWJxWKtjHEz2DW8Mg88QUIiJrwuK0YgnRNaPO/x1gcRIRWQsWpxWrvc6Zer4IeZoKwWmIiAhgcVq1wNYe6BPSBrLMLfiIiKwFi9PK/aV2uvZgjuAkREQEsDit3vgeAZAkYF/mFWRfKRcdh4jI4bE4rZy/tzv6dfQBAPzKUScRkXAsThvw53Qtr3MSEYnG4rQBcVEBcJKAgxc0OF9YKjoOEZFDY3HagHZKBQZ19gXAUScRkWgsThuR0LNmuvaXA7zOSUQkEovTRsR1D4Crs4TjecU4ebFYdBwiIofF4rQRKk9XDA9vB4CjTiIikVicNiShZyAAYO2BHMiyLDgNEZFjYnHakNHd/OHu6oTzhWU4lK0RHYeIyCGxOG2Il8IFt3XzB8DpWiIiUVicNub2q9O1/zuYC4OB07VERJbG4rQxw8PbQalwQa6mAmnnL4uOQ0TkcFicNsbd1RnjotQAgLUHsgWnISJyPCxOG1S7unbdoTxU6Q2C0xARORYWpw0a3LktfFu5oai0EttOF4iOQ0TkUFicNsjF2Ql/ia4Zdf68n9O1RESWxOK0Ubf3qinOjUcvoqyyWnAaIiLHweK0Ub2DWyPYxwNllXr8fixfdBwiIofB4rRRkiRhYs/2AIC16ZyuJSKyFBanDZvUu2a6NvnEJVwurRSchojIMbA4bVgXPyUiA7xRbZDx2+E80XGIiBwCi9PGTby6SGgNp2uJiCyCxWnjEnoGQpKAPeeKkHOlXHQcIiK7x+K0cYGtPdCvow+AmnM6iYioZbE47cCk3jWra9dwMwQiohbH4rQD46MC4ObshON5xTiWqxUdh4jIrrE47YDK0xWjIvwAcNRJRNTSWJx2ona6dtXeC1i97wJ2nimEngddExGZndUWZ2JiIvr27QulUgk/Pz9MmjQJJ06cqPOe6dOnQ5KkOo8BAwYISixWlV4PCUBRaSWe+v4A7vnXLgx5dxPWH84VHY2IyK5YbXGmpKRg9uzZ2LVrF5KSklBdXY2xY8eitLS0zvvi4uKQm5trfKxbt05QYnHWH87FE9+l4/rxZZ6mAo8t28fyJCIyIxfRARqyfv36Ol8vXboUfn5+2Lt3L4YNG2Z8XqFQQK1WWzqe1dAbZLz2y9EbShMAZAASgNd+OYoxkWo4O0kWTkdEZH+sdsR5PY1GAwDw8fGp83xycjL8/PwQHh6OGTNmID+/8ZNCdDodtFptnYct23OuCLmaigZflwHkaiqw51yR5UIREdkxmyhOWZYxb948DBkyBFFRUcbn4+Pj8e2332LTpk344IMPkJqailGjRkGn0zX4WYmJiVCpVMZHcHCwJf4ntJj84oZLsynvIyKixkmyLFv90svZs2fj119/xbZt2xAUFNTg+3JzcxESEoIVK1ZgypQp9b5Hp9PVKVatVovg4GBoNBp4e3ubPXtL23mmEPf8a9dN3/fdjAEY2LmtBRIREdkerVYLlUp1S11gtdc4az3++ONYu3YttmzZ0mhpAkBAQABCQkJw6tSpBt+jUCigUCjMHVOYfqE+CFC5I09TUe91TgmAWuWOfqE+9bxKRESmstqpWlmWMWfOHPz000/YtGkTQkNDb/o9hYWFyMrKQkBAgAUSWgdnJwkLEiIB1JRkfRYkRHJhEBGRmVhtcc6ePRvLli3D8uXLoVQqkZeXh7y8PJSX15wAUlJSgmeeeQY7d+5ERkYGkpOTkZCQAF9fX0yePFlwesuKiwrAkvtjoFa53/Day3/phrgox/mLBBFRS7PaqdolS5YAAEaMGFHn+aVLl2L69OlwdnbGoUOH8PXXX+PKlSsICAjAyJEjsXLlSiiVSgGJxYqLCsCYSDX2nCtCfnEFvt55HnvPX8alkkrR0YiI7IrVFufN1ix5eHhgw4YNFkpjG5ydJOMCIDdnJ+w9fxlr9mfjmbFdOVVLRGQmVjtVS80zqpsfvN1dkKupwK6zhaLjEBHZDRannVK4OCOhZyAA4Md9FwSnISKyHyxOOzYlpub2nfWH81CqqxachojIPrA47VhMh9bo2NYTZZV6rD+cJzoOEZFdYHHaMUmSjKNOTtcSEZkHi9POTYmpOeB659lCXLhcJjgNEZHtY3HauaA2nhjUuS1kGVi9L1t0HCIim8fidAB39qmZrv1h34Wb3h9LRESNY3E6gLgoNbzcnHG+sAxp5y+LjkNEZNNYnA7A080FE6Jr9qv9IY2LhIiImoPF6SDuuLq69tdDuSiv1AtOQ0Rku1icDqJvRx908PFEia4aG47wnk4ioqZqseLMysrCQw891FIfTyZycpKMo85Ve7MEpyEisl0tVpxFRUX46quvWurjqQlq7+nccYb3dBIRNVWTjxVbu3Zto6+fPXu2qR9NLSTYxxODu7TF9tOF+HFvNp4cHSY6EhGRzWlycU6aNAmSJDV6X6Ak8QxIa/PXPsHYfroQq/Zm4fFRXeDEczqJiEzS5KnagIAA/PjjjzAYDPU+9u3bZ86cZCZxUWoo3V1w4XI5z+kkImqCJhdnnz59Gi3Hm41GSQx3V2fcfvWczu/TuEiIiMhUTS7OZ599FoMGDWrw9S5dumDz5s1N/XhqQXfFBgMAfjucB015leA0RES2pcnFOXToUMTFxTX4upeXF4YPH97Uj6cWFB2kQld/JXTVBvxyIEd0HCIim8INEByQJEn4a+zVezo5XUtEZBIWp4Oa3Ls9XJwkHLigwfE8reg4REQ2g8XpoNq2UuC2bn4AgO9TufE7EdGtYnE6sLv71iwSWr3/AnTV3PidiOhWmLU4t2/fDp1OZ86PpBY0LKwd1N7uuFxWhaSjF0XHISKyCWYtzvj4eGRnZ5vzI6kFuTg74a6ri4RWpnKREBHRrTBrcXLDA9vz19hgSBKw9VQBsoq48TsR0c3wGqeDC/bxxJAuvgC4kxAR0a1o8ibvAPD111/X+bq6uho//fQT/Pz8jM898MADzfkRZAF39w3G1lMFWJV2AU/eFgYXZ/59ioioIc0qzqVLl9b5uqqqCj/88AM8PDwA1Nxoz+K0fmMi/dHG0xV52gpsOXUJoyL8RUciIrJazSrO6/eiVSqVWL58OTp16tSsUGRZChdnTIkJwr+3ncOKPVksTiKiRnBOjgD8eU/nH8fzka+tEJyGiMh6sTgJABDur0SfkDbQG2Ss2sudhIiIGmLW4nzxxRfh4+Njzo8kC7qnXwcAwIrUTBgMvLWIiKg+Zi3O+fPno3Xr1ub8SLKgCT0CoHR3QVZRObadLhAdh4jIKnGqlow83JwxpXd7AMB3ezIFpyEisk4sTqrjnv4107VJRy8iv5iLhIiIrtfs4iwuLjZHDrISEWpvxHRojWqDjB+4SIiI6AbNLs6hQ4ciLy/PHFnqSExMRN++faFUKuHn54dJkybhxIkTdd4jyzJeffVVBAYGwsPDAyNGjMCRI0fMnsXRGBcJ7cniIiEious0uzhjY2PRv39/HD9+vM7z+/fvx/jx45v8uSkpKZg9ezZ27dqFpKQkVFdXY+zYsSgtLTW+57333sOHH36IRYsWITU1FWq1GmPGjOEouJn+Eh0IpbsLMovKsP0MFwkREV2r2cX55Zdf4qGHHsKQIUOwbds2nDx5EnfddRdiY2OhUCia/Lnr16/H9OnT0b17d/Ts2RNLly5FZmYm9u7dC6BmtPnxxx/jpZdewpQpUxAVFYWvvvoKZWVlWL58eXP/Zzk0DzdnTL66SGj5bi4SIiK6llkWBy1YsABPP/00xowZg6ioKJSXlyM1NRWrV682x8cDADQaDQAY7xM9d+4c8vLyMHbsWON7FAoFhg8fjh07djT4OTqdDlqtts6DbnTv1UVCG49exEXuJEREZNTs4szNzcUTTzyBN954A5GRkXB1dcXUqVMRExNjjnwAakaX8+bNw5AhQxAVFQUAxuuq/v5191X19/dv9JprYmIiVCqV8REcHGy2nPYkQu2N2Ks7CfGQayKiPzW7ODt16oStW7di1apV2Lt3L3766SfMmjUL7777rjnyAQDmzJmDgwcP4rvvvrvhNUmS6nwty/INz11r/vz50Gg0xkdWFkuhIfcNqBl1frcnE9V6g+A0RETWodnFuXTpUuzfvx8TJkwAAIwbNw6bN2/GwoULMWvWrGYHfPzxx7F27Vps3rwZQUFBxufVajUA3DC6zM/Pv2EUei2FQgFvb+86D6pffFQA2ni6IldTgc0nLomOQ0RkFZpdnFOnTr3huZiYGOzYsQPJyclN/lxZljFnzhz89NNP2LRpE0JDQ+u8HhoaCrVajaSkJONzlZWVSElJwaBBg5r8c+lP7q7OuCu2Zip72a7zgtMQEVmHZhVneXk5tm3bhqNHj97wmlqtxt///vcmf/bs2bOxbNkyLF++HEqlEnl5ecjLy0N5eTmAminauXPn4u2338bq1atx+PBhTJ8+HZ6enrj33nub/HOprtp7OrecuoTMwjLBaYiIxGtycZ48eRLdunXDsGHD0KNHD4wYMQK5ubnG1zUaDZ5++ukmB1uyZAk0Gg1GjBiBgIAA42PlypXG9zz33HOYO3cuZs2ahdjYWGRnZ2Pjxo1QKpVN/rlUV0dfLwwN84UsA8u5fy0RUdOL8/nnn0ePHj2Qn5+PEydOwNvbG4MHD0Zm5p9/uMpy03edkWW53sf06dON75EkCa+++ipyc3NRUVGBlJQU46pbMp/7B4QAAL5Py4KuWi84DRGRWE0uzh07duDtt9+Gr68vunTpgrVr1yI+Ph5Dhw7F2bNnAdy44pVs020RflB7u6OotBK/HTL/9opERLakycVZXl4OFxeXOs99+umnuP322zF8+HCcPHmy2eHIOrg4OxmvdX69M0NsGCIiwZpcnBEREUhLS7vh+U8++QQTJ07E7bff3qxgZF3u6RcMFycJ+zKv4HC2RnQcIiJhmlyckydPrndDAgBYtGgR7rnnnmZd4yTr4uftjriomntnv9nJW1OIyHFJsoO3m1arhUqlgkaj4WYIN5GaUYS/frYT7q5O2D1/NFSerqIjERGZhSldYJZN3skxxIa0QYRaiYoqA1bt5VaFROSYWJx0yyRJwgMDOwIAvtl1nodcE5FDYnGSSSb1rjnk+nxhGbac4v61ROR4WJxkEk83F9zZp2azfS4SIiJHZNbi3LdvHyorK835kWSFpl3dSWjTiXycLywVnIaIyLLMWpx9+/ZFRkaGOT+SrFCndq0wPLwdZBn4mqNOInIwZi1OB7+zxaFMH9wRQM3+taW6arFhiIgsiNc4qUmGh7VDqK8Xiiuq8dP+bNFxiIgshsVJTeLkJOGBgTXXOr/akcHZBiJyGCxOarI7+wTBy80Zp/NLsP10oeg4REQWweKkJlO6uxpvTfnvjgyxYYiILITFSc3ywKCOAIA/jl9EZmGZ2DBERBbA4qRm6XzNrSlf8axOInIAZi3OBQsWwNfX15wfSTbgb1dvTVmZmoXiiiqxYYiIWpjZi9PHx8ecH0k2YHh4O3Txa4USXTVWpV0QHYeIqEVxqpaaTZIk46jzvzsyoOepKURkx1icZBZTegdB5eGKzKIy/H7soug4REQthsVJZuHh5ox7+3cAAPxn2znBaYiIWo7JxVleXo7s7Bu3WDty5IhZApHtemBgCFycJOw+V4TD2RrRcYiIWoRJxfnDDz8gPDwc48ePR3R0NHbv3m18bdq0aWYPR7YlQOWB8T0CAAD/2c5RJxHZJ5OK880338S+fftw4MAB/Oc//8FDDz2E5cuXA+DJKFTjoSGhAIBfDuTgorZCcBoiIvNzMeXNVVVVaNeuHQAgNjYWW7ZswZQpU3D69GlIktQiAcm29ApujdiQNkg7fxlf7cjAc3ERoiMREZmVSSNOPz8/HDx40Ph127ZtkZSUhGPHjtV5nhzbI0M7AQC+3Z2Jzcfz8XN6NnaeKeRtKkRkFyTZhDnWCxcuwMXFBWq1+obXtm/fjsGDB5s1nCVotVqoVCpoNBp4e3uLjmMX9AYZAxL/wKViXZ3nA1TuWJAQibioAEHJiIjqZ0oXmDTiDAoKuqE0i4uLAcAmS5NaRtLRvBtKEwDyNBV4bNk+rD+cKyAVEZF5NPs+zqFDhyIvL88cWcgO6A0yXvvlaL2v1U5tvPbLUU7bEpFZXLhchv/74xQul1Za7Gc2uzhjY2PRv39/HD9+vM7z+/fvx/jx45v78WRj9pwrQq6m4dW0MoBcTQX2nCuyXCgislv/2ZaBD5NOYt736Rb7mc0uzi+//BIPPfQQhgwZgm3btuHkyZO46667EBsbC4VCYY6MZEPyi2/tFpRbfR8RUUM0ZVVYkZoJAJg+ONRiP9ek21EasmDBAri5uWHMmDHQ6/UYN24cUlNTERMTY46PJxvip3Q36/uIiBqybPd5lFXqEaFWYliY5Y60bPaIMzc3F0888QTeeOMNREZGwtXVFVOnTmVpOqh+oT4IULmjobt6JdSsru0XyuPniKjpKqr0WLo9AwDw6PBOFt1LoNnF2alTJ2zduhWrVq3C3r178dNPP2HWrFl49913zZGPbIyzk4QFCZEA0GB5LkiIhLMTN8wgoqZbvT8bBSU6BKrc8ZfoQIv+7GYX59KlS7F//35MmDABADBu3Dhs3rwZCxcuxKxZs5odkGxPXFQAltwfA7XqxunYd+7owfs4iahZDAYZ/9pyFkDNNp+uzpY96KvZP23q1Kk3PBcTE4MdO3YgOTm5WZ+9ZcsWJCQkIDAwEJIkYc2aNXVenz59OiRJqvMYMGBAs34mmUdcVAC2PT8K380YgI/v7okwv1YAgMyiMsHJiMjWJR27iLMFpVC6u2Bqvw4W//ktVtMdO3bE9u3bm/UZpaWl6NmzJxYtWtTge+Li4pCbm2t8rFu3rlk/k8zH2UnCwM5tMal3EJ4Z1xUA8M3O8yjRVQtORkS27Iuro837B4SglcIsa1xN0qI/sU2bNs36/vj4eMTHxzf6HoVCUe8WgGRdxnTzRydfL5wtKMWKPZnG/WyJiEyRmlGEvecvw83ZCX8b1FFIBstODLeA5ORk+Pn5ITw8HDNmzEB+fn6j79fpdNBqtXUe1PKcnCT8fVhNWf572zlUVhsEJyIiW7Qk+QwA4I4+7eHnLea2NrMU5/bt26HT6W7455YWHx+Pb7/9Fps2bcIHH3yA1NRUjBo1qtGfn5iYCJVKZXwEBwdbJCsBk2Paw0+pQK6mAmvSs0XHISIbcyxXi03H8+EkAY8O6ywsh1mKMz4+HtnZ2Tf8c0u7++67MWHCBERFRSEhIQG//fYbTp48iV9//bXB75k/fz40Go3xkZWVZZGsBChcnPHw1YOuP0s5w/1qicgkn6XUjDbjewSgo6+XsBxmKc5rTyYz4ZQyswsICEBISAhOnTrV4HsUCgW8vb3rPMhy7hsQAm93F5y9VIqNR3g4ABHdmszCMvxyIAcA8NhwcaNNwA6ucV6rsLAQWVlZCAjgfYLWqpXCBdOvXtBfnHxG6F+0iMh2fLH1DAwyMDy8HaLaq4RmseriLCkpQXp6OtLT0wEA586dQ3p6OjIzM1FSUoJnnnkGO3fuREZGBpKTk5GQkABfX19MnjxZbHBq1PTBofBwdcahbA22nioQHYeIrFx+cQW+T7sAAHhshNjRJmDlxZmWlobevXujd+/eAIB58+ahd+/eeOWVV+Ds7IxDhw5h4sSJCA8Px4MPPojw8HDs3LkTSqVScHJqjI+XG6b2q1mUtTj5tOA0RGTt/rMtA5XVBsR0aI3+VrDPteXvHDXBiBEjGp3K27BhgwXTkDnNGNoJy3adx66zRdiXeRkxHZp3zy8R2acrZZX4ZmcGAGDWiC4W3cy9IVY94iT7FdjaA5N6tQcAfLqJo04iqt9/d2SgtFKPbgHeuK2bn+g4AFicJNBjIzrDSQL+OJ6PIzka0XGIyMoUV1QZjw6bM9I6RpuAmYrzxRdfhI+Pzw3/TNSYTu1aGY8D+nQzR51EVNeyXZnQlFehUzsvxEVZz9aqZinO+fPno3Xr1jf8M9HNzB7ZBQDw2+E8nLpYLDgNEVmL8ko9/r2tZjP32SO6WNUZvpyqJaG6qpWI666GLHPUSUR/WpGaiYKSSgT7eOD2XpY9qPpmWJwk3JxRNaPOtQdykFFQKjgNEYmmq9bj85Sa0ebM4Z0tflD1zTQ7Dfd6peaKaq/CyK7tYJD/PPmAiBzXqrQLyNNWwN9bgTv7BImOc4NmF2dERARefvlllJZypEBNN2dUGADgx30XkFVUJjgNEYlSWW3A4quXbR4b3hkKF2fBiW7U7OJMSkrCxo0bERYWhqVLl5ojEzmgPiFtMKSLL6oNMhZz1EnksFbtzUKOpgJ+SgWm9usgOk69ml2cgwYNwu7du/HOO+/glVdeQe/evZGcnGyGaORonhxdM+pclZaFC5c56iRyNDWjzZq/OM8c3hnurtY32gTMuDjogQcewMmTJ5GQkIAJEyZg8uTJOH2aqyTp1vXt6GMcdX66maNOIkfz474LyL5SjnZKBe7tb52jTcDMq2plWcbYsWPx97//HWvXrkVUVBSefvppFBfz/jy6NRx1EjmmKr3BeEuaNY82ATMU52effYaHH34Y0dHRUKlUGD16NLZv347Zs2dj8eLFSE9PR2RkJNLS0syRl+wcR51EjumnfRdw4XI5fFspcJ8VjzYBQJKbeZJwcHAwBgwYYHzExsZCoVDUec/bb7+N5cuX4/Dhw80K2xK0Wi1UKhU0Gg28vb1FxyEAqRlF+OtnO+HiJCH52REIauMpOhIRtaDKagNGfZCMC5fL8Y8J3fDI0E4Wz2BKFzT7WLFbuY/z4Ycfxssvv9zcH0UOonbUue10AT7dfBqJU6JFRyKiFrRqbxYuXK65tnlf/xDRcW7KItsx+Pn5YdOmTZb4UWQn5hqvdV5AZiGvdRLZq4oqPRZdPVpw1ojO8HCz3mubtSxSnJIkYfjw4Zb4UWQnYjv6YFh4O1QbZCz845ToOETUQlamZiFXUwG1tzvusdL7Nq9nXRsAEl3j6THhAIDV+y/gdH6J4DREZG4VVXrjStrZo7pY9Uraa7E4yWr1DG6NMZH+MMjgqJPIDi3bdR75xTq0b+2Bu2ODRce5ZSxOsmrzro46fzmQg+N5WsFpiMhcyiqrjYc6PHFbF7i52E4d2U5SckjdArwxIToAAPBR0knBaYjIXJZuz0BhaSU6+HhiSoz1nYDSGBYnWb2nRofBSQI2HLmIgxeuiI5DRM2kKavCZyk1o82nxoRZ3XmbN2NbackhdfFTYlLv9gCA9zecEJyGiJrrsy1nUFxRja7+Stzes73oOCZjcZJNeGp0OFydJWw9VYAdpwtExyGiJsovrsDS7ecAAM+M6wpnJ0lwItOxOMkmBPt44t6r93i9u+EEmrlTJBEJsmjTaVRUGRDToTVGd/MTHadJWJxkM+aMCoOnmzMOZF3BxqMXRcchIhNlFZXhuz2ZAIBnx0VAkmxvtAmwOMmGtFMq8NDgUADAPzecgN7AUSeRLfno95Oo0ssYGuaLgZ3bio7TZCxOsil/H94JrT1dcSq/BKv3Z4uOQ0S36Fiu1vjf7LPjugpO0zwsTrIp3u6ueGx4ZwDAhxtPoKJKLzgREd2Kd9cfhywDE6IDEB3UWnScZmFxks15cFBHBKjckaOpwNc7M0THIaKb2HGmAMknLsHFScKzY217tAmwOMkGubs6G7fiW7TpNK6UVQpOREQNMRhkvPPbcQDAvf07oKOvl+BEzcfiJJs0JSYIEWoltBXVWHx1v0sisj7rDufi4AUNvNyc8cRtYaLjmAWLk2ySs5OE5+MjAAD/3Z6BC5d52DWRtamsNhh3+5oxrBN8WykEJzIPFifZrBHh7TCoc1tU6g34cCM3gCeyNst3n8f5wjL4tnLDjKGdRMcxGxYn2SxJkjA/vhsAYHV6Ng5nawQnIqJamvIq4zm6T44Oh5fCRXAi82Fxkk3rEaTCxF6BkGXgrV+PYcfpAvycno2dZwq5QQKRQJ9uPo3LZVXo4tcK9/S1nUOqb4X9/BWAHNaz47ri14O52Hm2EDvPFhqfD1C5Y0FCJOKiAgSmI3I8WUVl+O/2DADAi+Mj4GJjx4bdjH39ryGHdDhbg+p6Rpd5mgo8tmwf1h/OFZCKyHG9s/44KvUGDO7SFiO72uZG7o2x6uLcsmULEhISEBgYCEmSsGbNmjqvy7KMV199FYGBgfDw8MCIESNw5MgRMWFJCL1Bxmu/HK33tdoqfe2Xo5y2JbKQvecv49eDuZAk4KXxkTa7kXtjrLo4S0tL0bNnTyxatKje19977z18+OGHWLRoEVJTU6FWqzFmzBgUFxdbOCmJsudcEXI1FQ2+LgPI1VRgz7kiy4UiclCyLOOtX2v+IvvXPkGIDPQWnKhlWPU1zvj4eMTHx9f7mizL+Pjjj/HSSy9hypQpAICvvvoK/v7+WL58OR599FFLRiVB8osbLs2mvI+Imm7tgRzsy7wCD1dnPG0HW+s1xKpHnI05d+4c8vLyMHbsWONzCoUCw4cPx44dOxr8Pp1OB61WW+dBtstP6W7W9xFR05RVVhu31ps9sjP8ve33vzmbLc68vDwAgL+/f53n/f39ja/VJzExESqVyvgIDravZdKOpl+oDwJU7mjoKoqEmtW1/UJ9LBmLyOF8lnIWuZoKBLXxwCN2tNlBfWy2OGtdf+FZluVGL0bPnz8fGo3G+MjKymrpiNSCnJ0kLEiIBIB6y1MGsCAhEs5O9rdAgchaZF8px+cpNXtGvzi+G9xdnQUnalk2W5xqtRoAbhhd5ufn3zAKvZZCoYC3t3edB9m2uKgALLk/BmrVjVND/t4KjIpo+N8HImq+xHXHoKs2oH+oD+Kj1KLjtDibLc7Q0FCo1WokJSUZn6usrERKSgoGDRokMBmJEBcVgG3Pj8J3MwZg4dRe+PeDsWjr5YaLWh3P7CRqQXvOFeF/V28/eSXBPm8/uZ5Vr6otKSnB6dOnjV+fO3cO6enp8PHxQYcOHTB37ly8/fbbCAsLQ1hYGN5++214enri3nvvFZiaRHF2kjCwc1vj18/HReC5Hw9i4e+nMLFXe7RT2sfJDETWolpvwIK1NffOT+0bjO6BKsGJLMOqR5xpaWno3bs3evfuDQCYN28eevfujVdeeQUA8Nxzz2Hu3LmYNWsWYmNjkZ2djY0bN0KpVIqMTVbizj5B6NFehWJdNd5bf1x0HCK7s3xPJo7laqHycMWz4yJEx7EYSZZlh95SRavVQqVSQaPR8HqnHdp7/jLuWFJze9KPjw1Cn5A2ghMR2YfCEh1G/jMZ2opqvDGxO6YN7Cg6UrOY0gVWPeIkaq4+IW1wV2wQAOCVnw9z6z0iM3lv/QloK6oRGeCNe/uHiI5jUSxOsnvPx0XA290FR3K0WL77vOg4RDZvf+ZlrEyruZXvjUndHe52LxYn2b22rRR4dlzN9l/vbziBghKd4EREtktvkI0Lgu6ICUKfEMfbXITFSQ7h3v4h6B7oDW1FNd79jQuFiJpq+e7zOHhBA6XCBS/EO86CoGuxOMkhODtJeH1iFABg1d4LSM3gaSlEpsovrsB7G04AAJ6N6+qwt3ixOMlh9Alpg6l9a/Ymfmn1IVRWGwQnIrItb/16DMUV1YgOUuE+B1sQdC0WJzmUF+Ij0NbLDScvluDLbWdFxyGyGdtOFeDn9Bw4ScBbk3o43IKga7E4yaG09nTDSxO6AQD+749TyCoqE5yIyPpVVOnx8s+HAQAPDOyIHkGOsUNQQ1ic5HAm926PgZ3aoqLKgJd/PgwH3wOE6KY+TzmLcwWl8FMqMG9suOg4wrE4yeFIkoQ3J0fBzdkJyScuYd2hhs9vJXJ0p/OL8enmmj3DX0mIhLe7q+BE4rE4ySF1btcKM0d0BgC8+ssRaMqqBCcisj4Gg4wXfjyESr0BoyL8MKFHgOhIVoHFSQ5r9sjO6NzOC5eKdXhr3VHRcYiszvI9mUg7fxlebs54Y1KUQxwZditYnOSwFC7OePeOaADA92kXsP10geBERNYjT1OBd65uFvLsuK5o39pDcCLrweIkhxbb0QfTBtTcjzb/p0Mor9QLTkQknizLePnnwyjRVaNXcGubP/nE3Fic5PCei+uKAJU7MovK8NHvJ0XHIRJu3aE8JB29CBcnCe/c4dj3bNaHxUkOT+nuijcn1WzH9+XWs0jPuiI2EJFAhSU64z2bs0Z0RoSa5xRfj8VJBOC2bv64vWcgDDLw7KoDqKjilC05pgVrj6CotBIRaiXmjAoTHccqsTiJrnrt9u7wbeWGU/klWPjHKdFxiCxu/eFc/O9gLpydJLx/Z0+4ubAi6sPfCtFVbbzc8OakHgCAz1POcMqWHMrl0kr8Y03NFO3M4Z0cflu9xrA4ia4RF6XmlC05pAVrj6CgpBJhfq3wxG2com0Mi5PoOjVTtgqcyi/Bx79zypbs368Hc7H2QM3JJ+//tScULs6iI1k1FifRddp4ueGtyTWrbL/Ycga7zxVi55lC/JyejZ1nCqE3cFN4sh/52gr8Y80hAMDskV3QK7i12EA2wEV0ACJrNK67GnfEBOHHfRdw77921ynLAJU7FiREIi6K+3aSbZNlGS/8dAiXy6rQPdAbj3MV7S3hiJOoAUO6tAWAG0aYeZoKPLZsH9YfzhURi8hsVqZmYdPxfLi5OOGju3txFe0t4m+JqB56g4z3Npyo97XaGn3tl6OctiWblVlYhjf+V3O4wbNjuyLcXyk4ke1gcRLVY8+5IuRqKhp8XQaQq6nAnnNFlgtFZCZVegOeXLkfpZV69Ovog4eGhIqOZFNYnET1yC9uuDSb8j4ia/LJH6ewP/MKlO4u+PDuntyL1kQsTqJ6+Cndzfo+Imux51wRFm0+DQB4a3IPBLXxFJzI9rA4ierRL9QHASp3NPb38ACVO/qF+lgsE1Fzacqr8NTKdBhkYEpMe9zeM1B0JJvE4iSqh7OThAUJkQDQYHnOj4/gFBfZDFmW8Y81h5F9pRwdfDzx+sQo0ZFsFouTqAFxUQFYcn8M1Kq607G1XbmLC4PIhqxMzcIvB3Lg7CTh46m90ErB2/ibir85okbERQVgTKQae84VIb+4An5Kd1RW6/Hg0lQs352JgZ3aIoHTXWTljudpsWDtEQDAM2O7IqZDG8GJbBuLk+gmnJ0kDOzcts5zs0d2xqebz2D+T4cQ1V6FUF8vQemIGldWWY3Z3+6DrtqA4eHt8OiwTqIj2TxO1RI1wVOjw9Gvow9KdDV/KPEUFbJWL685gjOXSuHvrcCHd/WEE6/LNxuLk6gJXJyd8H/39EZbLzcczdXizV+Pio5EdINVaVn4cd8FOEnAwqm90baVQnQku8DiJGoitcodH93dC5IELNuViZ/Ts0VHIjI6kqMxHkw9d3Q4BnRqe5PvoFvF4iRqhmHh7TBnZBcAwAs/HsLxPK3gRESApqwKjy2rua45suuf/46Sedh0cb766quQJKnOQ61Wi45FDmbu6HAMDfNFeZUeM7/ZC21FlehI5MAMBhnzvk9HZlEZgtp44KO7e/G6ppnZdHECQPfu3ZGbm2t8HDp0SHQkcjDOThIWTu2N9q09kFFYhnkrD8DAU1NIkCUpZ/DH1aPCPru/D1p7uomOZHdsvjhdXFygVquNj3bt2omORA7Ix8sNS+6PgZuLE34/dhGLk0+LjkQOaPPxfPxzY81xeG9M7I6o9irBieyTzRfnqVOnEBgYiNDQUEydOhVnz54VHYkcVHRQa7wxsTsA4IOkk/jj2EXBiciRnLlUgidW7IcsA/f064C7+3YQHclu2XRx9u/fH19//TU2bNiAf/3rX8jLy8OgQYNQWFjY4PfodDpotdo6DyJzubtvB9zXvwNkGXhyRTpOXSwWHYkcgLaiCjO+TkNxRTViQ9rgtdu7i45k12y6OOPj43HHHXegR48eGD16NH799VcAwFdffdXg9yQmJkKlUhkfwcHBlopLDmJBQnf0D63ZHGHG12m4UlYpOhLZMb1BxtwV6Th7qRQBKncsub8P3Fxs+o92q2dXv10vLy/06NEDp06davA98+fPh0ajMT6ysrIsmJAcgZuLExbfF2NcLDRn+X5U6w2iY5Gden/DCWw6ng+FixO+mBaLdkpuctDS7Ko4dTodjh07hoCAgAbfo1Ao4O3tXedBZG5tWynw5YOx8HRzxrbTBXj9f0dRrTdg55lC/JyejZ1nCqHnyltqpu9Ts/BZyhkAwHt3RqNHEBcDWYJNb/L+zDPPICEhAR06dEB+fj7efPNNaLVaPPjgg6KjEaFbgDc+vKsXHvt2L77eeR5r9mdDW1FtfD1A5Y4FCZGIi2r4L3pEDdlxpgAvrq65/e6J28IwsVd7wYkch02POC9cuIB77rkHXbt2xZQpU+Dm5oZdu3YhJCREdDQiAEBclBqTe9f8gXZtaQJAnqYCjy3bh/WHc0VEIxt25lIJZn6zF9UGGQk9A/HU6DDRkRyKTY84V6xYIToCUaP0Bhk7ThfU+5oMQALw2i9HMSZSDWfu7kK3oLBEh4f+mwptRTViOrTG+3dGQ5L4744l2fSIk8ja7TlXhDytrsHXZQC5mgrsOVdkuVBks8oqq/HQf1NxvrBmO70vHoiFu6uz6FgOh8VJ1ILyiyvM+j5yXNV6A2Z/uw8HLmjQ2tMVXz3UD748JkwIFidRC/JTupv1feSYZFnGi6sPYfOJS1C4OOHfD/ZF53atRMdyWCxOohbUL9QHASp3NHYFSu2tQL9QH4tlItvzYdJJfJ9WcyD1J/f0Rp+QNqIjOTQWJ1ELcnaSsCAhEgAaLM8OPp6NFis5tn9tOYtPNtUcGvD6xCiM7c6jE0VjcRK1sLioACy5PwZqVd3p2DaernCSgD0Zl/Hyz4chy9wQger6bk8m3lp3DADw7LiuuH8Ab7WzBjZ9OwqRrYiLCsCYSDX2nCtCfnEF/JTu6Bfqg/8dzMHclen4dncmvBQumB8fwVsLCADwy4Ec4wYHjw7rhFkjOgtORLVYnEQW4uwkYWDntnWem9irPcoq9Zj/0yF8seUsvNxc8CRvZnd4SUcv4qmV6cYjwl7gX6isCqdqiQS7p18H/GNCNwDAR7+fxKebeQi2I/v96EXM+rZmV6DbewbizUlRLE0rw+IksgKPDO2EZ8aGA6g57YLl6Zg2Hb+IWd/uQ5VexoToAHx4V0/uKGWFWJxEVmLOqDA8PYbl6ag2n8jHzG/2oVJvwPgeanx8dy+4OPOPaGvEa5xEVuTx28IgScA/N57E+xtOwGCQMWdUFxhk3LCwiCMR+7H+cB4e/65mpBnXXY2FU3vDlaVptVicRFZmzqgwSJKE9zecwAdJJ3EoW4ODF67U2fOWR5LZjzX7s/H0qgPQG2SM78HStAX8f4fICs0e2cW4YGjj0Ys3bBTPI8nsw3d7MvHU9+nQG2TcEROE/2Np2gT+P0Rkpf42OBQqj/onhWq3Snjtl6PQG7hxgq2RZRmfpZzB/J8OQZaBaQNC8P6d0bymaSP4/xKRldpzrgia8uoGX+eRZLbJYJDx+v+O4p3fjgMAHh3eCa9P7A4nXrO2GbzGSWSleCSZ/dFV6zHv+wP49WDNFPs/JnTDI0M7CU5FpmJxElkpHklmX66UVeLRb/Zi97kiuDpL+OCuXri9Z6DoWNQELE4iK1V7JFmepgINXcX08XTlkWQ24OylEjz8VRrOFZSilcIFn0/rg8FdfEXHoibiNU4iK3UrR5JdKa/Csl3nebKKFdt+ugCTPt2OcwWlaN/aAz8+NoilaeNYnERWrKEjydTeCgzo5AODDCxYewT/WHMYVXoDAEBvkLHzTCF+Ts/GzjOFXHUriCzL+GpHBh74zx5oK6oR06E1fp4zGF3VStHRqJkk2cH/qqrVaqFSqaDRaODt7S06DlG99Ab5hp2DnCTgiy1n8c7645BlIDakDe7o0x7/98dp5Gr+XDDEzRIsr7xSjxdXH8Lq/dkAgIm9AvHuHdFwd3UWnIwaYkoXsDhZnGTj/jh2EXNXpqO4ov5bV2qneZfcH8PytIDMwjI8umwvjuVq4ewk4YW4CDwyNJQnnFg5U7qAU7VENu62bv5YM3swXBq4D5CbJVjOukO5mPDJVhzL1aKtlxuWPdwfM4Z1YmnaGa6qJbID+VodqhspxWs3S7j+MG1qvvJKPV7/31F8tycTANAruDWW3B+DAJWH4GTUElicRHaAmyWIcyxXiydX7MfJiyUAgJnDO+PpseHcc9aOsTiJ7EBTNkuob8ERjyq7ddV6Az5LOYOFf5xClV6GbysFPrq7J4aGtRMdjVoYi5PIDtzKZglOElBepQcArD+ci9d+OcrVt010Or8YT39/AAcuaAAAo7v5I3FKD7RTKgQnI0vgqlquqiU7sf5wLh5btg8AGixPABjYyQc7z964MTxX395cRZUen6WcweLNZ1CpN8Db3QWv3t4dk3u35wIgG8dVtUQOqKHNEgJU7lg4tSf+NrgjJAn1libA1bc3s+1UAeIXbsXHv59Cpd6AkV3bYeNTwzElJoil6WA4VUtkR+KiAjAmUl3vtcuJvYLQsa0XFqw90uD3166+3XWmEE5OEq9/AsgqKsM7648bTzTxUyqwIKE7xvdQszAdFIuTyM44O0kN3nLS2tP1lj5j9vJ9uFJeZfzaEa9/aiuq8Onm01i6LQOVegMkCXhwYEc8PTYcSvdb+z2SfWJxEjmQW119e21pAkCepgKPLdvnENc/yyv1WLbrPJaknEFRaSUAYHCXtnhpfCQiA7kOglicRA7lVlbf1kdGzeKhV9cegdLdFQUlOrubwq2o0mP57kwsTj6DghIdAKBTOy+8NL4bRkX4cVqWjLiqlqtqycHc6urbWxGgcsfLE7qhjZfCZq+HFpVWYtmu8/h6ZwYKSmpGmEFtPPDEqDBMjmnPjQwcBDd5NwGLkxxRffdxtvZwvWGKtil8vFwxuVd7jI5UW3WJHs/TYtmu8/hh7wVUVNUcyda+tQfmjOqCO2KC4ObCwnQkLE4TsDjJUV2/c5BBlnHfl7vN+jNqS3RUhD8gQfgUb6muGv87mIPv9mQhPeuK8fmo9t74+7DOGB+lhgtHmA7JlC7gNU4iB3X96lu9QW7S9c/GFJVW4d/bM/Dv7Rl1nvd0dUL3QG8EtvaALMvI11ZgV8aVOu8Z0wn4198nNDtDRZUeySfy8cuBXPxx/KJxdOniJGFMpD+mDQzBwE5teQ2TbpldjDgXL16M999/H7m5uejevTs+/vhjDB069Ja+lyNOoj+Z8/qnuWS8Y3p55lwpR/KJS0g5mY9tpwpQWqk3vhbq64W7+wbjjpggbpFHRg41Vbty5UpMmzYNixcvxuDBg/H555/jyy+/xNGjR9GhQ4ebfj+Lk6iu+q5/itZYeRoMMs4WlGLf+ctIO1+EtPOXcfZSaZ33tG/tgb9EB+Av0YGIau/N0SXdwKGKs3///oiJicGSJUuMz3Xr1g2TJk1CYmLiTb+fxUl0o2uvf/p6KfD0qgO4qDXfFK6pRnUE3rpnFHKuVCDnSjmyr5TjdH4JTl4sxqmLJcbN62s5SUDvDm0wPLwdRnRthx7tVSxLapTDXOOsrKzE3r178cILL9R5fuzYsdixY0e936PT6aDT6Yxfa7XaFs1IZIuuv/756u2ReGzZPkgQM4W7KQMYmLipwdcVLk7oGdwafULaIDakDfqEtEFrTzfLBSSHYtPFWVBQAL1eD39//zrP+/v7Iy8vr97vSUxMxGuvvWaJeER2o3YDeZFTuM5OEtTe7mjf2gOBrd0R0tYLEWoluqqVCGnrZbW3vZD9senirHX9FIwsyw1Oy8yfPx/z5s0zfq3VahEcHNyi+YjsQX0byF8urcQbv1qmTE++Gc9yJKtg08Xp6+sLZ2fnG0aX+fn5N4xCaykUCigUXElH1BT1bSA/LqqmTJOO5mFNeo5xf1dzGtMJLE2yGjZ9p6+bmxv69OmDpKSkOs8nJSVh0KBBglIROZbaMn0loTtSXxqN72YMwEODO8LHy3zXGM1xPyeRudj0iBMA5s2bh2nTpiE2NhYDBw7EF198gczMTMycOVN0NCKHU1uiAzu3xUsTIuuszE3NKMIXW8+irFJ/8w+6RlPu4yRqSTZfnHfffTcKCwvx+uuvIzc3F1FRUVi3bh1CQkJERyNyaNdP6w4O88Xjt4Vh15lCbD9zCdmXywGgxXcOIjI3m7+Ps7l4HycREZnSBTZ9jZOIiMjSWJxEREQmYHESERGZgMVJRERkAhYnERGRCVicREREJmBxEhERmYDFSUREZAIWJxERkQlYnERERCaw+b1qm6t2x0GtVis4CRERiVLbAbeyC63DF2dxcTEA8DBrIiJCcXExVCpVo+9x+E3eDQYDcnJyoFQqIUm2f1CuVqtFcHAwsrKyuGn9dfi7qR9/Lw3j76Z+9vh7kWUZxcXFCAwMhJNT41cxHX7E6eTkhKCgINExzM7b29tu/oU2N/5u6sffS8P4u6mfvf1ebjbSrMXFQURERCZgcRIREZmAxWlnFAoFFixYAIVCITqK1eHvpn78vTSMv5v6OfrvxeEXBxEREZmCI04iIiITsDiJiIhMwOIkIiIyAYuTiIjIBCxOB6HT6dCrVy9IkoT09HTRcYTKyMjAww8/jNDQUHh4eKBz585YsGABKisrRUcTYvHixQgNDYW7uzv69OmDrVu3io4kVGJiIvr27QulUgk/Pz9MmjQJJ06cEB3LKiUmJkKSJMydO1d0FIticTqI5557DoGBgaJjWIXjx4/DYDDg888/x5EjR/DRRx/hs88+w4svvig6msWtXLkSc+fOxUsvvYT9+/dj6NChiI+PR2ZmpuhowqSkpGD27NnYtWsXkpKSUF1djbFjx6K0tFR0NKuSmpqKL774AtHR0aKjWJ5Mdm/dunVyRESEfOTIERmAvH//ftGRrM57770nh4aGio5hcf369ZNnzpxZ57mIiAj5hRdeEJTI+uTn58sA5JSUFNFRrEZxcbEcFhYmJyUlycOHD5effPJJ0ZEsiiNOO3fx4kXMmDED33zzDTw9PUXHsVoajQY+Pj6iY1hUZWUl9u7di7Fjx9Z5fuzYsdixY4egVNZHo9EAgMP9+9GY2bNnY8KECRg9erToKEI4/Cbv9kyWZUyfPh0zZ85EbGwsMjIyREeySmfOnMEnn3yCDz74QHQUiyooKIBer4e/v3+d5/39/ZGXlycolXWRZRnz5s3DkCFDEBUVJTqOVVixYgX27duH1NRU0VGE4YjTBr366quQJKnRR1paGj755BNotVrMnz9fdGSLuNXfy7VycnIQFxeHv/71r3jkkUcEJRfr+uP0ZFm2iyP2zGHOnDk4ePAgvvvuO9FRrEJWVhaefPJJLFu2DO7u7qLjCMMt92xQQUEBCgoKGn1Px44dMXXqVPzyyy91/hDU6/VwdnbGfffdh6+++qqlo1rUrf5eav+Dz8nJwciRI9G/f3/897//vekZfPamsrISnp6eWLVqFSZPnmx8/sknn0R6ejpSUlIEphPv8ccfx5o1a7BlyxaEhoaKjmMV1qxZg8mTJ8PZ2dn4nF6vhyRJcHJygk6nq/OavWJx2rHMzExotVrj1zk5ORg3bhx++OEH9O/f3y7PIb1V2dnZGDlyJPr06YNly5Y5xH/s9enfvz/69OmDxYsXG5+LjIzExIkTkZiYKDCZOLIs4/HHH8fq1auRnJyMsLAw0ZGsRnFxMc6fP1/nub/97W+IiIjA888/7zDT2bzGacc6dOhQ5+tWrVoBADp37uzQpZmTk4MRI0agQ4cO+Oc//4lLly4ZX1Or1QKTWd68efMwbdo0xMbGYuDAgfjiiy+QmZmJmTNnio4mzOzZs7F8+XL8/PPPUCqVxuu9KpUKHh4egtOJpVQqbyhHLy8vtG3b1mFKE2BxkgPauHEjTp8+jdOnT9/wFwhHm4C5++67UVhYiNdffx25ubmIiorCunXrEBISIjqaMEuWLAEAjBgxos7zS5cuxfTp0y0fiKwOp2qJiIhM4FirIYiIiJqJxUlERGQCFicREZEJWJxEREQmYHESERGZgMVJRERkAhYnERGRCVicREREJmBxEhERmYDFSUREZAIWJ5GD+u677+Du7o7s7Gzjc4888giio6Oh0WgEJiOybtyrlshBybKMXr16YejQoVi0aBFee+01fPnll9i1axfat28vOh6R1eLpKEQOSpIkvPXWW7jzzjsRGBiIhQsXYuvWrSxNopvgiJPIwcXExODIkSPYuHEjhg8fLjoOkdXjNU4iB7ZhwwYcP34cer0e/v7+ouMQ2QSOOIkc1L59+zBixAh8+umnWLFiBTw9PbFq1SrRsYisHq9xEjmgjIwMTJgwAS+88AKmTZuGyMhI9O3bF3v37kWfPn1ExyOyahxxEjmYoqIiDB48GMOGDcPnn39ufH7ixInQ6XRYv369wHRE1o/FSUREZAIuDiIiIjIBi5OIiMgELE4iIiITsDiJiIhMwOIkIiIyAYuTiIjIBCxOIiIiE7A4iYiITMDiJCIiMgGLk4iIyAQsTiIiIhOwOImIiEzw/7otqcDOE0S5AAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def f(x):\n",
        "  return x**2 - 2*x + 1\n",
        "\n",
        "def f_prime(x):\n",
        "  h = 0.01\n",
        "  return (f(x+h)-f(x)) / h\n",
        "\n",
        "def f_prime2(x):\n",
        "  return 2*x - 2\n",
        "\n",
        "all_x = tf.linspace(-5,5,100)\n",
        "all_y = f(all_x)\n",
        "\n",
        "x = tf.Variable(-4.) # initial value\n",
        "LR = 0.1\n",
        "\n",
        "x_history, y_history = [], []\n",
        "for _ in range(100):\n",
        "  y = f(x)\n",
        "  step = LR*f_prime(x)\n",
        "\n",
        "  x_history.append(x.numpy())\n",
        "  y_history.append(y.numpy())\n",
        "\n",
        "  x.assign_sub(step)  # x = x - lr*dx\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5,5))\n",
        "ax.plot(all_x, all_y)\n",
        "ax.scatter(x_history, y_history)\n",
        "plt.xlabel('$x$')\n",
        "plt.ylabel('$y=x^2-2x+1$')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bea_5I3I0ZNi"
      },
      "source": [
        "# 과제 001\n",
        "\n",
        "주어진 함수를 이용하여 gradient descent 를 직접 구현하고 그래프를 그리시오.\n",
        "\n",
        "\n",
        "**제출**\n",
        "\n",
        "구현한 코드가 담긴 colab 파일을 PLATO \"텐서플로 과제 제출 01\"에 제출하세요. (23/7/6까지)\n",
        "\n",
        "```\n",
        "def f(x):\n",
        "    return x**4 - 3*x**3 +2\n",
        "\n",
        "def fprime(x):\n",
        "    h = 0.001\n",
        "    return (f(x+h) - f(x))/h\n",
        "```\n",
        "\n",
        "* 힌트: 교과서 44p를 **참고**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a3tV2kTk3Yqb"
      },
      "source": [
        "Chapter03 회귀(Regression)\n",
        "\n",
        "05 평균 제곱 오차 손실함수\n",
        "\n",
        "06 넘파이 단순 선형 회귀\n",
        "\n",
        "07 자동 미분 계산\n",
        "\n",
        "08 텐서플로 단순 선형 회귀\n",
        "\n",
        "09 다변수 선형 회귀\n",
        "\n",
        "10 tf.keras.optimizers를 이용한 학습\n",
        "\n",
        "11 다항식 회귀\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9ZWR-b55Kjq"
      },
      "source": [
        "# ch3. regression\n",
        "\n",
        "통계에서 선형 회귀는 스칼라 응답(e.g.,키)와 하나 이상의 설명 변수(e.g.,몸무게) 간의 관계를 모델링하기 위한 선형 접근 방식입니다.\n",
        "\n",
        "주로 어떤 값을 예측하기 위해 많이 사용합니다.\n",
        "\n",
        "예를 들어, 100명의 키와 몸무게 데이터가 엑셀과 같이 row, column의 테이블 데이터 형식으로 주어질 때, 몸무게를 입력하면 키를 예측하거나 그 반대의 경우를 예측하는 경우에 사용할 수 있습니다.\n",
        "\n",
        "* simple linear regression: 몸무게 하나로 키를 예측할 때\n",
        "* multiple linear regression: 몸무게와 나이로 키를 예측할 때\n",
        "\n",
        "[선형회귀](https://en.wikipedia.org/wiki/Linear_regression)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fbaUQMqDcd2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "E6hE1RFTOmk3"
      },
      "source": [
        "\n",
        "# 3.1 mean squared error"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "plrcAI0qM54F"
      },
      "source": [
        "\n",
        "\n",
        "$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y-\\hat{y})^2$$\n",
        "\n",
        "**왜 MSE 인가?**\n",
        "\n",
        "* 1차원이면\n",
        "\n",
        "> 미분이 안되거나, loss 의 합이 0인 경우에 잘못된 결과를 도출할 수 있음\n",
        "\n",
        "* 3 차원 이상이면\n",
        "\n",
        "> overfit 될 수 있고, 여러 개의 local optimal points 존재할 수 있음\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scMY5STt28Ya"
      },
      "outputs": [],
      "source": [
        "def MSE(y, t):\n",
        "  return tf.reduce_mean(tf.square(y - t))   # (y - t)**2\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError\n",
        "# Computes the mean of squares of errors between labels and predictions.\n",
        "tf_MSE = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "t = tf.constant([1, 2, 3, 4], dtype=tf.float32)\n",
        "\n",
        "y1 = tf.constant([0.5, 1, 1.5, 2])\n",
        "y2 = tf.constant([0.5, 1.5, 2.5, 3.5])\n",
        "\n",
        "print(\"MSE(t, y1)\", MSE(t, y1))\n",
        "print(\"MSE(t, y2)\", MSE(t, y2))\n",
        "\n",
        "print(\"tf_MSE(t, y1)\", tf_MSE(t, y1))\n",
        "print(\"tf_MSE(t, y2)\", tf_MSE(t, y2))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eLYfkA2FTaqE"
      },
      "source": [
        "# 3.2 automatic differentiation\n",
        "\n",
        "* 자동으로 미분하기 위해 forward pass 동안 어떤 연산이 어떤 순서로 발생하는지 기억해야 함\n",
        "\n",
        "* 그런 다음 backward pass 중에 gradient 를 계산하기 위해 이 연산 목록을 역순으로 순회함\n",
        "\n",
        "* 이를 위해 tf.GradientTape 을 사용함\n",
        "\n",
        "[자동미분](https://www.tensorflow.org/guide/autodiff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4-z64FdXnAg"
      },
      "outputs": [],
      "source": [
        "x = tf.Variable(2.0) # tf.Variable(2.0, trainable=True)\n",
        "y = tf.Variable(3.0) # tf.Variable(3.0, trainable=True)\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
        "# Record operations for automatic differentiation.\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    z = x**2 + y**2\n",
        "\n",
        "# dx = 2x -> 4\n",
        "# dy = 2y -> 6\n",
        "dx, dy = tape.gradient(z, [x, y])\n",
        "print('dx=', dx.numpy())\n",
        "print('dy=', dy.numpy())\n",
        "\n",
        "x = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x**2\n",
        "\n",
        "# dy = 2x * dx => 6\n",
        "dy_dx = tape.gradient(y, x)\n",
        "print('dy_dx.numpy()', dy_dx.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaDND0z-U-JM"
      },
      "outputs": [],
      "source": [
        "# multiple variables\n",
        "W = tf.Variable(tf.random.normal((3, 2)), name='w')\n",
        "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
        "x = [[1., 2., 3.]]\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  # Since python >= 3.5 the @ operator is supported (see PEP 465).\n",
        "  # In TensorFlow, it simply calls the tf.matmul() function\n",
        "  y = x @ W + b\n",
        "  loss = tf.reduce_mean(y**2)\n",
        "\n",
        "[dl_dW, dl_db] = tape.gradient(loss, [W, b])\n",
        "\n",
        "print('W', W)\n",
        "print('b', b)\n",
        "print('dl_dw', dl_dW)\n",
        "print('dl_db', dl_db)\n",
        "\n",
        "my_vars = {\n",
        "    'W': W,\n",
        "    'b': b\n",
        "}\n",
        "\n",
        "grad = tape.gradient(loss, my_vars)\n",
        "print('grad[W]', grad['W'])\n",
        "print('grad[b]', grad['b'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4FSLtAhk6BRE"
      },
      "source": [
        "# 3.3 MSE with SGD\n",
        "\n",
        "gradient descent 는 계산량이 많아 데이터가 많으면 느리다 (보통 GPU 메모리가 부족함 ㅠㅠ)\n",
        "\n",
        "* f 의 도함수를 구한 후 관찰값의 개수만큼 모두 계산함 (예 1,000개의 센서 데이터)\n",
        "* $x_{1}, x_{2}, x_{3}...$ 등으로 n 개의 features 로 f 값을 계산 시 n개 각각 계산해야 함\n",
        "* epochs이 100 번이라면, 1000 * n * 100 미분\n",
        "\n",
        "반면에, Stochastic Gradient Descent 는\n",
        "\n",
        "* **실제 기울기(전체 데이터 세트에서 계산)를 추정치(데이터 세트에서 임의로 하나 선택하여 계산)로 대체하여 근사함**\n",
        "\n",
        "* 고차원 최적화 문제에서 계산 부담을 줄여줌\n",
        "\n",
        "* but, zig-zag 패턴 및 수렴 속도가 엄청(?) 느려질 수 있고, 하나 고른 데이터가 outlier 일 경우에 영향을 많이 받을 수 있음\n",
        "\n",
        "* mini-batch 전략으로 보완함\n",
        "\n",
        "[확률적 경사 하강법](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
        "\n",
        "<br>\n",
        "\n",
        "| Stochastic | Batch |\n",
        "| ----------- | ----------- |\n",
        "| <img src=\"https://www.mltut.com/wp-content/uploads/2020/04/Untitled-document-5.png\"> | <img src=\"https://www.mltut.com/wp-content/uploads/2020/04/Untitled-document-4.png\"> |\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "<img src=\"https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/48_blog_image_2.png\">\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kbP1fkMFrcjwBO3NVP9OfQ.png\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "id": "V_eMWI0zGAyq",
        "outputId": "ae8c5ab6-c659-497c-cc00-7df5c4acd09a"
      },
      "outputs": [],
      "source": [
        "#Mini Batch Gradient Descent\n",
        "def f(x, w, b):\n",
        "  return w*x + b\n",
        "\n",
        "def MSE(y, t):\n",
        "  return tf.reduce_mean((y-t)**2)\n",
        "\n",
        "def w_grad_MSE(y, t, w, b):\n",
        "  return 2*tf.reduce_mean((y-t_batch)*x_batch)\n",
        "\n",
        "def b_grad_MSE(y, t, w, b):\n",
        "  return 2*tf.reduce_mean((y-t_batch))\n",
        "\n",
        "x = tf.range(-10,10, dtype=tf.float32)\n",
        "t = tf.range(-10,10, dtype=tf.float32)\n",
        "\n",
        "print('x', x)\n",
        "print('t', t)\n",
        "\n",
        "w = tf.Variable(tf.random.normal([]))\n",
        "b = tf.Variable(tf.random.normal([]))\n",
        "\n",
        "#w = tf.Variable(0.5)\n",
        "#b = tf.Variable(0.0)\n",
        "\n",
        "print('w', w)\n",
        "print('b', b)\n",
        "\n",
        "lr = 0.01  # 0.01, learning rate\n",
        "loss_list = []\n",
        "\n",
        "train_size = tf.size(t) # 12\n",
        "batch_size = 4\n",
        "\n",
        "# mini batch SGD\n",
        "K = train_size // batch_size # 3\n",
        "\n",
        "for epoch in range(100):\n",
        "    loss = 0\n",
        "    for step in range(K):\n",
        "        mask = np.random.choice(train_size, batch_size)\n",
        "        #print(mask)\n",
        "        # https://www.tensorflow.org/api_docs/python/tf/gather\n",
        "        # Gather slices from params axis axis according to indices.\n",
        "        x_batch = tf.gather(x, indices=mask)\n",
        "        t_batch = tf.gather(t, indices=mask)\n",
        "\n",
        "        y_batch = f(x_batch, w, b) #w*x_batch + b\n",
        "        loss = MSE(y_batch, t_batch) # calculate MSE\n",
        "        dw = w_grad_MSE(y_batch, t_batch, w, b)\n",
        "        db = b_grad_MSE(y_batch, t_batch, w, b)\n",
        "        #dw = 2*tf.reduce_mean((y-t_batch)*x_batch) # gradients\n",
        "        #db = 2*tf.reduce_mean((y-t_batch))\n",
        "        #print('dw', dw)\n",
        "        #print('db', db)\n",
        "        w.assign_sub(lr*dw)\n",
        "        b.assign_sub(lr*db)\n",
        "\n",
        "    loss /= tf.cast(K, dtype=tf.float32)   # average loss\n",
        "    loss_list.append(loss.numpy())\n",
        "    if not epoch%10:\n",
        "        print(\"epoch={}: w={:>8.4f}. b={:>8.4f}, loss={:>8.4f}\".format(epoch, w.numpy(), b.numpy(), loss.numpy()))\n",
        "\n",
        "print(\"w={:>8.4f}. b={:>8.4f}, loss={:>8.4f}\".format(w.numpy(), b.numpy(), loss.numpy()))\n",
        "\n",
        "plt.plot(loss_list)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "6QEEtOLa3Gom",
        "outputId": "f8714f5d-915b-4d8e-d3c6-3ea9f52dd4d9"
      },
      "outputs": [],
      "source": [
        "#Mini Batch Gradient Descent\n",
        "def f(x, w, b):\n",
        "  return w*x + b\n",
        "\n",
        "def MSE(y, t):\n",
        "  return tf.reduce_mean((y-t)**2)\n",
        "\n",
        "x = tf.range(-10, 10, dtype=tf.float32)\n",
        "t = tf.range(-10, 10, dtype=tf.float32)\n",
        "\n",
        "print('x', x)\n",
        "print('t', t)\n",
        "\n",
        "w = tf.Variable(tf.random.normal([]))\n",
        "b = tf.Variable(tf.random.normal([]))\n",
        "\n",
        "#w = tf.Variable(0.5)\n",
        "#b = tf.Variable(0.0)\n",
        "\n",
        "print('w', w)\n",
        "print('b', b)\n",
        "\n",
        "lr = 0.01  # 0.01, learning rate\n",
        "loss_list = []\n",
        "\n",
        "train_size = tf.size(t) # 12\n",
        "batch_size = 4\n",
        "\n",
        "# mini batch SGD\n",
        "K = train_size // batch_size # 3\n",
        "\n",
        "for epoch in range(100):\n",
        "    loss = 0\n",
        "    for step in range(K):\n",
        "        mask = np.random.choice(train_size, batch_size)\n",
        "        #print(mask)\n",
        "        # https://www.tensorflow.org/api_docs/python/tf/gather\n",
        "        # Gather slices from params axis axis according to indices.\n",
        "        x_batch = tf.gather(x, indices=mask)\n",
        "        t_batch = tf.gather(t, indices=mask)\n",
        "        with tf.GradientTape() as tape:\n",
        "          y_batch = f(x_batch, w, b) #w*x_batch + b\n",
        "          loss = MSE(y_batch, t_batch) # calculate MSE & loss_mean\n",
        "\n",
        "        [dw, db] = tape.gradient(loss, [w,b])\n",
        "        # dW = 2*tf.reduce_mean((y-t_batch)*x_batch) # gradients\n",
        "        # dB = 2*tf.reduce_mean((y-t_batch))\n",
        "        #print('dW', dW)\n",
        "        #print('dB', dB)\n",
        "        w.assign_sub(lr*dw)\n",
        "        b.assign_sub(lr*db)\n",
        "\n",
        "    loss /= tf.cast(K, dtype=tf.float32)   # average loss\n",
        "    loss_list.append(loss.numpy())\n",
        "    if not epoch%10:\n",
        "        print(\"epoch={}: w={:>8.4f}. b={:>8.4f}, loss={:>8.4f}\".format(epoch, w.numpy(), b.numpy(), loss.numpy()))\n",
        "\n",
        "print(\"w={:>8.4f}. b={:>8.4f}, loss={:>8.4f}\".format(w.numpy(), b.numpy(), loss.numpy()))\n",
        "\n",
        "plt.plot(loss_list)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qif2NTMDfnQw"
      },
      "source": [
        "## 3.3.1 tf.keras optimizers\n",
        "\n",
        "### SGD: Gradient descent (with momentum) optimizer.\n",
        "\n",
        "$$\\vec{v_{t}} = \\rho\\cdot\\vec{v_{t-1}} - lr\\cdot\\vec{dw}$$\n",
        "$$w = w + v_{t}$$\n",
        "\n",
        "```python\n",
        "tf.keras.optimizers.SGD(\n",
        "    learning_rate=0.01,  \n",
        "    momentum=0.0,        \n",
        "    ...\n",
        ")\n",
        "```\n",
        "### Adagrad\n",
        "\n",
        "learning rate 에 지금까지 계산한 gradient 제곱값의 누적합(G)를 나눠서 갱신함\n",
        "\n",
        "기울기가 급격하게 변화할 때 lr 값이 작아지고, 반대일 때 커진다.\n",
        "\n",
        "학습을 많이할 수록 learning rate가 점점 작아짐\n",
        "\n",
        "$$w = w - lr\\cdot\\frac{1}{\\sqrt{G+\\epsilon}}\\cdot dw$$\n",
        "\n",
        "```python\n",
        "tf.keras.optimizers.Adagrad(\n",
        "    learning_rate=0.001,\n",
        "    epsilon=1e-07,\n",
        "    ...\n",
        ")\n",
        "```\n",
        "\n",
        "### RMSprop\n",
        "\n",
        "Adagrad의 G를 구할 때, 직전 단계의 G 누적합에 $\\rho$ 값으로 반영 비율을 조절함\n",
        "\n",
        "$$G_{t}\\prime = \\rho \\cdot G_{t-1} + (1-\\rho)\\cdot (dw_{t})^2$$\n",
        "\n",
        "$$w = w - lr\\cdot\\frac{1}{\\sqrt{G\\prime+\\epsilon}}\\cdot dw$$\n",
        "\n",
        "```python\n",
        "tf.keras.optimizers.experimental.RMSprop(\n",
        "    learning_rate=0.001,\n",
        "    rho=0.9,\n",
        "    ...\n",
        ")\n",
        "```\n",
        "\n",
        "### Adam\n",
        "\n",
        "RMSprop 방법에 momentum 을 추가한 방법임\n",
        "\n",
        "현재 가장 널리 사용되고 있는 최적화 알고리즘\n",
        "\n",
        "```python\n",
        "tf.keras.optimizers.Adam(\n",
        "    learning_rate=0.001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    ...\n",
        ")\n",
        "\n",
        "```\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/11681225/50016682-39742a80-000d-11e9-81da-ab0406610b9c.gif\">\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/11681225/49325458-fc785480-f585-11e8-8d2a-9012d6024c6e.gif\">\n",
        "\n",
        "[멋진 설명(33-50p)](https://www.slideshare.net/yongho/ss-79607172) - Kakao 하용호님\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/45377884/91630397-18838100-ea0c-11ea-8f90-515ef74599f1.png\">\n",
        "\n",
        "\n",
        "\n",
        "[Animation 참조](https://github.com/ilguyi/optimizers.numpy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "1hj5EPe1gLe1",
        "outputId": "c7d916da-6e25-4ad5-e920-24b843e60ffd"
      },
      "outputs": [],
      "source": [
        "x = tf.Variable(2.0)\n",
        "y = tf.Variable(3.0)\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental/SGD\n",
        "# Gradient descent (with momentum) optimizer.\n",
        "opt = tf.keras.optimizers.SGD(learning_rate=0.1) # learning_rate=0.001\n",
        "#opt = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
        "#opt = tf.keras.optimizers.RMSprop(learning_rate=0.1)\n",
        "\n",
        "loss_list = [ ]\n",
        "for epoch in range(100):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = x**2 + y**2\n",
        "  loss_list.append(loss.numpy())\n",
        "  #if loss < 0.001: break\n",
        "  dx, dy = tape.gradient(loss, [x, y])\n",
        "  grads_and_vars = zip([dx, dy], [x, y])\n",
        "  opt.apply_gradients(grads_and_vars)\n",
        "\n",
        "  if not epoch%10:\n",
        "    print(\"epoch={}: w={:>8.4f}. b={:>8.4f}, loss={:>8.4f}\".format(epoch, x.numpy(), y.numpy(), loss.numpy()))\n",
        "\n",
        "\n",
        "print(\"w={:>8.4f}. b={:>8.4f}, loss={:>8.4f}\".format(x.numpy(), y.numpy(), loss.numpy()))\n",
        "\n",
        "plt.plot(loss_list)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3Y4SMgOa4YC"
      },
      "outputs": [],
      "source": [
        "# def MSE(y, t):\n",
        "#     return tf.reduce_mean((y-t)**2)\n",
        "\n",
        "tf_MSE = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "\n",
        "x = tf.range(20, dtype=tf.float32)\n",
        "t = tf.range(20, dtype=tf.float32)\n",
        "\n",
        "w = tf.Variable(0.5)\n",
        "b = tf.Variable(0.0)\n",
        "\n",
        "lr = 0.001  # 0.01, learning rate\n",
        "loss_list = []\n",
        "\n",
        "train_size = tf.size(t) # 12\n",
        "batch_size = 4\n",
        "\n",
        "# mini batch SGD\n",
        "K = train_size // batch_size # 3\n",
        "\n",
        "for epoch in range(100):\n",
        "    loss = 0\n",
        "    for step in range(K):\n",
        "        mask = np.random.choice(train_size, batch_size)\n",
        "        #print(mask)\n",
        "        x_batch = tf.gather(x, indices=mask)\n",
        "        t_batch = tf.gather(t, indices=mask)\n",
        "        with tf.GradientTape() as tape:\n",
        "          y = w*x_batch + b                          # calculate the output\n",
        "          loss += tf_MSE(y, t_batch) # calculate MSE\n",
        "        optimizer.minimize(loss, [w, b], tape=tape)\n",
        "        #dW = 2*tf.reduce_mean((y-t_batch)*x_batch) # gradients\n",
        "        #dB = 2*tf.reduce_mean((y-t_batch))\n",
        "        #print('dW', dW)\n",
        "        #print('dB', dB)\n",
        "        #w.assign_sub(lr*dW)\n",
        "        #b.assign_sub(lr*dB)\n",
        "\n",
        "    loss /= tf.cast(K, dtype=tf.float32)   # average loss\n",
        "    loss_list.append(loss.numpy())\n",
        "    if not epoch%10:\n",
        "        print(\"epoch={}: w={:>8.4f}. b={:>8.4f}, loss={:>8.4f}\".format(epoch, w.numpy(), b.numpy(), loss.numpy()))\n",
        "\n",
        "print(\"w={:>8.4f}. b={:>8.4f}, loss={:>8.4f}\".format(w.numpy(), b.numpy(), loss.numpy()))\n",
        "\n",
        "plt.plot(loss_list)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OynmCv0DZiOt"
      },
      "source": [
        "# 3.4 simple linear regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "973J3NpjJ9rS",
        "outputId": "6883e9a7-13e2-4377-ef13-0e1cc1ca70ce"
      },
      "outputs": [],
      "source": [
        "# https://www.tensorflow.org/tutorials/keras/regression\n",
        "# Predict fuel efficiency with the Auto MPG dataset\n",
        "\n",
        "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
        "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
        "                'Acceleration', 'Model Year', 'Origin']\n",
        "\n",
        "raw_dataset = pd.read_csv(url, names=column_names,\n",
        "                          na_values='?', comment='\\t',\n",
        "                          sep=' ', skipinitialspace=True)\n",
        "MPG_dataset = raw_dataset.copy()\n",
        "MPG_dataset.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv_ktwsZPYX-",
        "outputId": "09b432f6-18aa-434e-add9-5689238dcf8a"
      },
      "outputs": [],
      "source": [
        "print(MPG_dataset.isna().sum())\n",
        "MPG_dataset = MPG_dataset.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "KmaVwB5pKO5t",
        "outputId": "6af1b495-559c-412c-d3ef-10f5c09f9156"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "train_dataset = MPG_dataset.sample(frac=0.8, random_state=0)\n",
        "test_dataset = MPG_dataset.drop(train_dataset.index)\n",
        "sns.pairplot(train_dataset[['MPG', 'Cylinders', 'Displacement', 'Weight']], diag_kind='kde')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "lyqdT_RbKjvt",
        "outputId": "e2550ec1-56a7-4e29-9a4f-b42de12f7795"
      },
      "outputs": [],
      "source": [
        "train_dataset.corr() # miles per gallon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_eeUGg0cgkR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lDIfwnHLTee"
      },
      "outputs": [],
      "source": [
        "def load_MPG_train_dataset(columns):\n",
        "  x = train_dataset[columns].to_numpy()\n",
        "  y = train_dataset[['MPG']].to_numpy()\n",
        "  return (x, y)\n",
        "\n",
        "def load_MPG_test_dataset(columns):\n",
        "  x = test_dataset[columns].to_numpy()\n",
        "  y = test_dataset[['MPG']].to_numpy()\n",
        "  return (x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "jxAPtkfUOZcc",
        "outputId": "08930bc8-3618-4cc7-872f-8df980bf4c35"
      },
      "outputs": [],
      "source": [
        "# gradient descent\n",
        "x, y = load_MPG_train_dataset(['Horsepower'])\n",
        "\n",
        "w = tf.Variable(tf.random.normal([]))\n",
        "b = tf.Variable(tf.random.normal([]))\n",
        "\n",
        "loss_list = []\n",
        "\n",
        "tf_MSE = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "\n",
        "for epoch in range(100):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_hat = w * x + b\n",
        "    loss = tf_MSE(y_hat, y)\n",
        "\n",
        "  dw, db = tape.gradient(loss, [w, b])\n",
        "  opt.apply_gradients([[dw, w], [db, b]])\n",
        "  loss_list.append(loss.numpy())\n",
        "\n",
        "  if not epoch%10:\n",
        "    print(\"epoch={}: w={:>8.4f}. b={:>8.4f}, loss={:>8.4f}\".format(epoch, w.numpy(), b.numpy(), loss.numpy()))\n",
        "\n",
        "print(\"w={:>8.4f}. b={:>8.4f}, loss={:>8.4f}\".format(w.numpy(), b.numpy(), loss.numpy()))\n",
        "\n",
        "plt.plot(loss_list)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "-PU_1IUxSpvP",
        "outputId": "c0b50249-8c4a-493b-863d-6db115864817"
      },
      "outputs": [],
      "source": [
        "# mini-batch gradient descent\n",
        "x, y = load_MPG_train_dataset(['Horsepower'])\n",
        "batch_size = 32\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "w = tf.Variable(tf.random.normal([]))\n",
        "b = tf.Variable(tf.random.normal([]))\n",
        "\n",
        "loss_list = []\n",
        "\n",
        "tf_MSE = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "for epoch in range(100):\n",
        "  dataset_train = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "  for step, (x_batch, y_batch) in enumerate(dataset_train):\n",
        "    #print('step', step)\n",
        "    #print('x_batch', x_batch.shape)\n",
        "    #print('y_batch', y_batch.shape)\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_hat = w * x + b\n",
        "      loss = tf_MSE(y_hat, y)\n",
        "\n",
        "    dw, db = tape.gradient(loss, [w, b]) #미분해줘\n",
        "    opt.apply_gradients([[dw, w], [db, b]])\n",
        "    loss/=batch_size\n",
        "    loss_list.append(loss.numpy())\n",
        "\n",
        "  if not epoch%10:\n",
        "    print(\"epoch={}: w={:>8.4f}. b={:>8.4f}, loss={:>8.4f}\".format(epoch, w.numpy(), b.numpy(), loss.numpy()))\n",
        "\n",
        "print(\"w={:>8.4f}. b={:>8.4f}, loss={:>8.4f}\".format(w.numpy(), b.numpy(), loss.numpy()))\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "axs[0].plot(loss_list)\n",
        "axs[0].set(xlabel=\"epochs\", ylabel=\"loss\")\n",
        "\n",
        "axs[1].scatter(x, y)\n",
        "axs[1].plot(x, w*x+b, color='g')\n",
        "axs[1].set(xlabel=\"Horsepower\", ylabel=\"MPG\")\n",
        "\n",
        "x, y = load_MPG_test_dataset(['Horsepower'])\n",
        "axs[2].scatter(x, y)\n",
        "axs[2].plot(x, w*x+b, color='r')\n",
        "axs[2].set(xlabel=\"Horsepower\", ylabel=\"MPG\")\n",
        "print(\"loss={:>8.4f}\".format(tf_MSE(w*x+b, y)))\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "i7GWC3OAZn66"
      },
      "source": [
        "# 3.5 multiple linear regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "2WEUCfGFc_51",
        "outputId": "91cb1386-182e-469c-8e71-469c5487e0a5"
      },
      "outputs": [],
      "source": [
        "# mini-batch gradient descent\n",
        "x, y = load_MPG_train_dataset(column_names[1:])\n",
        "batch_size = 32\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "w = tf.Variable(tf.random.normal(shape=(x.shape[1],1)))\n",
        "b = tf.Variable(tf.random.normal([]))\n",
        "\n",
        "loss_list = []\n",
        "\n",
        "tf_MSE = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "for epoch in range(100):\n",
        "  dataset_train = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "  for step, (x_batch, y_batch) in enumerate(dataset_train):\n",
        "    #print('step', step)\n",
        "    #print('x_batch', x_batch.shape)\n",
        "    #print('y_batch', y_batch.shape)\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_hat = x @ w + b\n",
        "      loss = tf_MSE(y_hat, y)\n",
        "\n",
        "    dw, db = tape.gradient(loss, [w, b])\n",
        "    opt.apply_gradients([[dw, w], [db, b]])\n",
        "    loss/=batch_size\n",
        "    loss_list.append(loss.numpy())\n",
        "\n",
        "  if not epoch%10:\n",
        "    print(\"epoch={}: loss={:>8.4f}\".format(epoch, loss.numpy()))\n",
        "\n",
        "print(\"loss={:>8.4f}\".format(loss.numpy()))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15, 5))\n",
        "ax.plot(loss_list)\n",
        "ax.set(xlabel=\"epochs\", ylabel=\"loss\")\n",
        "\n",
        "#axs[1].scatter(x, y)\n",
        "#axs[1].plot(x, w*x+b, color='g')\n",
        "#axs[1].set(xlabel=\"Horsepower\", ylabel=\"MPG\")\n",
        "\n",
        "x, y = load_MPG_test_dataset(column_names[1:])\n",
        "#axs[2].scatter(x, y)\n",
        "#axs[2].plot(x, w*x+b, color='r')\n",
        "#axs[2].set(xlabel=\"Horsepower\", ylabel=\"MPG\")\n",
        "print(\"loss={:>8.4f}\".format(tf.reduce_mean(tf_MSE(x@w+b, y))))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvg5fPQK2Fo6"
      },
      "source": [
        "# 과제 002\n",
        "\n",
        "주어진 보스톤 주택 가격 데이터 셋을 이용하여 주택 가격을 예측하는 multiple variables regression 을 수행하는 프로그램을 직접 작성하시오.\n",
        "\n",
        "단, 아래 조건을 만족하여 구현하시오.\n",
        "\n",
        "* loss 함수: tensorflow 의 MSE\n",
        "* gradient 계산: tensorflow의 GradientTape()\n",
        "* optimizer: tensorflow의 Adam\n",
        "\n",
        "**제출**\n",
        "\n",
        "구현한 코드가 담긴 colab 파일을 PLATO \"텐서플로 과제 제출 01\"에 제출하세요. (23/7/6까지)\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/datasets/boston_housing/load_data\n",
        "# features 설명 (http://lib.stat.cmu.edu/datasets/boston)\n",
        "# 보스톤 주택 가격(MEDV) 예측 (1인당 범죄율, 주택당 평균 방 개수, 학생대 교사 비율 등의 features 이용함)\n",
        "# MEDV(주택 가격 중앙값, 단위: $1,000)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data(\n",
        "    path='boston_housing.npz', test_split=0.2, seed=113\n",
        ")\n",
        "print('x_train.shape', 'y_train.shape', x_train.shape, y_train.shape)\n",
        "\n",
        "all_train_data = np.hstack((x_train, y_train.reshape((-1, 1))))\n",
        "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
        "df = pd.DataFrame(all_train_data, columns=column_names)\n",
        "df.head()\n",
        "sns.pairplot(df[['CRIM', 'ZN', 'INDUS', 'TAX', 'MEDV']], diag_kind='kde')\n",
        "```\n",
        "\n",
        "* 힌트:\n",
        "https://dschloe.github.io/python/tensorflow2.0/ch4_4_boston_housing_deeplearning/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
