{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVE-EXIwz8bl"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iyWPGkZmMAJn"
      },
      "source": [
        "##"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_BiBqFm52drj"
      },
      "source": [
        "# 2.7 linear algebra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r7f-gFB40DX"
      },
      "outputs": [],
      "source": [
        "# https://www.tensorflow.org/api_docs/python/tf/linalg\n",
        "# Operations for linear algebra.\n",
        "\n",
        "import tensorflow as tf\n",
        "#1 L2 norm (유클리디언 거리)\n",
        "# https://www.tensorflow.org/api_docs/python/tf/norm\n",
        "\n",
        "a = tf.constant([1, 2], dtype=tf.float32)\n",
        "print('a', a)\n",
        "print('tf.norm(a)', tf.norm(a)) # tf.linalg.norm(a)\n",
        "\n",
        "#2 transpose\n",
        "A = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
        "print('A', A)\n",
        "print('tf.linalg.matrix_transpose(A)', tf.linalg.matrix_transpose(A))\n",
        "\n",
        "#3 determinant\n",
        "print('tf.linalg.det(A)', tf.linalg.det(A))\n",
        "\n",
        "#4 inverse matrix\n",
        "B =  tf.linalg.inv(A)\n",
        "print('B', B)\n",
        "\n",
        "#5 matrix multiplication\n",
        "print('tf.matmul(A, B)', tf.matmul(A, B))  # tf.linalg.matmul(A, B)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rDWfJGOn5zUP"
      },
      "source": [
        "**matrix equation**\n",
        "\n",
        "Ax = b\n",
        "\n",
        "==> x = A<sup>-1</sup> x b\n",
        "\n",
        "A<sup>-1</sup> = $\\frac{1}{DET(A)}$ A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SLoqrqqM6wJ_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(-11.000001, shape=(), dtype=float32)\n",
            "x1 tf.Tensor(\n",
            "[[2.]\n",
            " [1.]], shape=(2, 1), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[2.]\n",
            " [1.]], shape=(2, 1), dtype=float32) tf.Tensor(\n",
            "[[2.]\n",
            " [1.]], shape=(2, 1), dtype=float32)\n",
            "(<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
            "array([[2.],\n",
            "       [1.]], dtype=float32)>, <tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
            "array([[2.],\n",
            "       [1.]], dtype=float32)>)\n"
          ]
        }
      ],
      "source": [
        "#1 Ax = b\n",
        "A = tf.constant([[1, 2],\n",
        "                 [3, -5]], dtype=tf.float32)\n",
        "\n",
        "b = tf.constant([[4],\n",
        "                 [1]], dtype=tf.float32)\n",
        "\n",
        "#2 DET(A)\n",
        "print(tf.linalg.det(A))\n",
        "\n",
        "def solve(A, b):\n",
        "  # x = A^-1 x b\n",
        "  x1 = tf.matmul(tf.linalg.inv(A), b)\n",
        "  print('x1', x1)\n",
        "\n",
        "  # https://www.tensorflow.org/api_docs/python/tf/linalg/solve\n",
        "  # Solves systems of linear equations.\n",
        "  x2 = tf.linalg.solve(A, b)  # x = 2, y = 1\n",
        "  print(x1, x2)\n",
        "\n",
        "  return x1, x2\n",
        "\n",
        "print(solve(A, b))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "l9xUGEXn8puD"
      },
      "source": [
        "**LU decomposition**\n",
        "\n",
        "A = LU 로 분해하여\n",
        "\n",
        "Ax = b 를 푸는 과정에 활용해 보면,\n",
        "\n",
        "=> LUx = b\n",
        "\n",
        "=> Ux = Z  로 놓고\n",
        "\n",
        "=> LZ = b  를 풀고\n",
        "\n",
        "=> Ux = z 를 풀면\n",
        "\n",
        "x 를 구할 수 있음\n",
        "(역행렬 방법과 비교하여 빠르고, 메모리 적게 사용해서 계산 가능)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pi3HiAi59fXt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.linalg.lu(A) tf.Tensor(\n",
            "[[ 3.         -5.        ]\n",
            " [ 0.33333334  3.6666667 ]], shape=(2, 2), dtype=float32) tf.Tensor([1 0], shape=(2,), dtype=int32)\n",
            "x tf.Tensor(\n",
            "[[2.]\n",
            " [1.]], shape=(2, 1), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "A = tf.constant([[1, 2],\n",
        "                 [3, -5]], dtype=tf.float32)\n",
        "\n",
        "b = tf.constant([[4],\n",
        "                 [1]], dtype=tf.float32)\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/linalg/lu\n",
        "# Computes the LU decomposition of one or more square matrices.\n",
        "\n",
        "lu, p = tf.linalg.lu(A)\n",
        "print('tf.linalg.lu(A)', lu, p)\n",
        "\n",
        "x = tf.linalg.lu_solve(lu, p, b)\n",
        "print('x', x)\n",
        "\n",
        "# L = tf.linalg.band_part(lu,-1,0) - tf.linalg.diag(tf.linalg.diag_part(lu)) + tf.linalg.diag(tf.ones(shape = lu.shape[0],))\n",
        "# U = tf.linalg.band_part(lu, 0, -1)\n",
        "# permu_operator = tf.linalg.LinearOperatorPermutation(p)\n",
        "# P = permu_operator.to_dense()\n",
        "\n",
        "# print('L', L)\n",
        "# print('U', U)\n",
        "\n",
        "# AA = tf.matmul(P, tf.matmul(L,U), transpose_a = True)\n",
        "# print('AA', AA)\n",
        "\n",
        "# AAA = tf.linalg.lu_reconstruct(lu, p)\n",
        "# print('AAA', AAA)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4SNn22puAZmm"
      },
      "source": [
        "**least squares**\n",
        "\n",
        "(0, 6), (1, 0), (2, 0) 을 지나는 가장 가까운 직선의 방정식은?\n",
        "\n",
        "6 = a x 0 + b\n",
        "\n",
        "0 = a x 1 + b\n",
        "\n",
        "0 = a x 2 + b\n",
        "\n",
        "위 수식을 모두 만족하는 a 는 존재하는가?\n",
        "\n",
        "=> 존재하지 않으면 **차이가 최소(에러가 최소)**가 되는 근사값 $\\hat{a}$ 을 구해보자!\n",
        "\n",
        "Error<sup>2</sup> = (6 - (a x 0 + b))<sup>2</sup> + (0 - (a x 1 + b))<sup>2</sup> + (0 - (a x 2 + b))<sup>2</sup>\n",
        "\n",
        "=> 행렬로 풀려면 공식 이용\n",
        "\n",
        "$X = \\begin{bmatrix}\\hat{a}\\\\\n",
        "\\hat{b}\n",
        "\\end{bmatrix}$\n",
        "\n",
        "(A<sup>T</sup>A)X = A<sup>T</sup>b\n",
        "\n",
        "(참고: https://en.wikipedia.org/wiki/Linear_least_squares)\n",
        "\n",
        "\n",
        "=> 2차 방정식이므로 $\\hat{a}$ 과 $\\hat{b}$ 에 대해 미분하자!\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tQYksr0Z_N8a"
      },
      "source": [
        "# 2.8 gradient decent\n",
        "\n",
        "수학에서 경사 하강법은 미분 가능 함수의 로컬 최소값을 찾기 위한 1차 반복 최적화 알고리즘입니다.\n",
        "\n",
        "알고리즘: 현재 지점에서 함수의 기울기의 반대 방향(빼줌)으로 반복되는 단계를 수행하는 것이며, 그 결과로 local minimum을 찾을 수 있습니다.\n",
        "\n",
        "$x = x - lr * dx$\n",
        "\n",
        "\n",
        "[gradient decent](https://en.wikipedia.org/wiki/Gradient_descent)\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1H77kMZ1UhsB3N7HRXJ77StzJzTyXodf0\" width=\"400\"/>\n",
        "\n",
        "[Animation](https://medium.com/onfido-tech/machine-learning-101-be2e0a86c96a)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mAdWjUPAA7sH"
      },
      "source": [
        "\n",
        "예)\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1vcxLliVPoM6Lw6HfFEQAFiXDf3J0CjL_\" width=\"200\"/>\n",
        "\n",
        "\n",
        "$ F = (x-1)^2$\n",
        "\n",
        "=> $x^2 -2x + 1$\n",
        "\n",
        "[2차 함수](https://www.geogebra.org/m/xgdQkfk3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "w7EJ3u3DETMt",
        "outputId": "fe4a8aff-0b10-4bf9-9fc5-2972040fdfe0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, '$y=x^2-2x+1$')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "def f(x):\n",
        "  return x**2 - 2*x + 1\n",
        "\n",
        "def f_prime(x):\n",
        "  h = 0.01\n",
        "  return (f(x+h)-f(x)) / h\n",
        "\n",
        "def f_prime2(x):\n",
        "  return 2*x - 2\n",
        "\n",
        "all_x = tf.linspace(-5,5,100)\n",
        "all_y = f(all_x)\n",
        "\n",
        "x = tf.Variable(-4.) # initial value\n",
        "LR = 0.1\n",
        "\n",
        "x_history, y_history = [], []\n",
        "for _ in range(100):\n",
        "  y = f(x)\n",
        "  step = LR*f_prime(x)\n",
        "\n",
        "  x_history.append(x.numpy())\n",
        "  y_history.append(y.numpy())\n",
        "\n",
        "  x.assign_sub(step)  # x = x - lr*dx\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5,5))\n",
        "ax.plot(all_x, all_y)\n",
        "ax.scatter(x_history, y_history)\n",
        "plt.xlabel('$x$')\n",
        "plt.ylabel('$y=x^2-2x+1$')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bea_5I3I0ZNi"
      },
      "source": [
        "# 과제 001\n",
        "\n",
        "주어진 함수를 이용하여 gradient descent 를 직접 구현하고 그래프를 그리시오.\n",
        "\n",
        "\n",
        "**제출**\n",
        "\n",
        "구현한 코드가 담긴 colab 파일을 PLATO \"텐서플로 과제 제출 01\"에 제출하세요. (23/7/6까지)\n",
        "\n",
        "```\n",
        "def f(x):\n",
        "    return x**4 - 3*x**3 +2\n",
        "\n",
        "def fprime(x):\n",
        "    h = 0.001\n",
        "    return (f(x+h) - f(x))/h\n",
        "```\n",
        "\n",
        "* 힌트: 교과서 44p를 **참고**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a3tV2kTk3Yqb"
      },
      "source": [
        "Chapter03 회귀(Regression)\n",
        "\n",
        "05 평균 제곱 오차 손실함수\n",
        "\n",
        "06 넘파이 단순 선형 회귀\n",
        "\n",
        "07 자동 미분 계산\n",
        "\n",
        "08 텐서플로 단순 선형 회귀\n",
        "\n",
        "09 다변수 선형 회귀\n",
        "\n",
        "10 tf.keras.optimizers를 이용한 학습\n",
        "\n",
        "11 다항식 회귀\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9ZWR-b55Kjq"
      },
      "source": [
        "# ch3. regression\n",
        "\n",
        "통계에서 선형 회귀는 스칼라 응답(e.g.,키)와 하나 이상의 설명 변수(e.g.,몸무게) 간의 관계를 모델링하기 위한 선형 접근 방식입니다.\n",
        "\n",
        "주로 어떤 값을 예측하기 위해 많이 사용합니다.\n",
        "\n",
        "예를 들어, 100명의 키와 몸무게 데이터가 엑셀과 같이 row, column의 테이블 데이터 형식으로 주어질 때, 몸무게를 입력하면 키를 예측하거나 그 반대의 경우를 예측하는 경우에 사용할 수 있습니다.\n",
        "\n",
        "* simple linear regression: 몸무게 하나로 키를 예측할 때\n",
        "* multiple linear regression: 몸무게와 나이로 키를 예측할 때\n",
        "\n",
        "[선형회귀](https://en.wikipedia.org/wiki/Linear_regression)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fbaUQMqDcd2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "E6hE1RFTOmk3"
      },
      "source": [
        "\n",
        "# 3.1 mean squared error"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "plrcAI0qM54F"
      },
      "source": [
        "\n",
        "\n",
        "$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y-\\hat{y})^2$$\n",
        "\n",
        "**왜 MSE 인가?**\n",
        "\n",
        "* 1차원이면\n",
        "\n",
        "> 미분이 안되거나, loss 의 합이 0인 경우에 잘못된 결과를 도출할 수 있음\n",
        "\n",
        "* 3 차원 이상이면\n",
        "\n",
        "> overfit 될 수 있고, 여러 개의 local optimal points 존재할 수 있음\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scMY5STt28Ya"
      },
      "outputs": [],
      "source": [
        "def MSE(y, t):\n",
        "  return tf.reduce_mean(tf.square(y - t))   # (y - t)**2\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError\n",
        "# Computes the mean of squares of errors between labels and predictions.\n",
        "tf_MSE = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "t = tf.constant([1, 2, 3, 4], dtype=tf.float32)\n",
        "\n",
        "y1 = tf.constant([0.5, 1, 1.5, 2])\n",
        "y2 = tf.constant([0.5, 1.5, 2.5, 3.5])\n",
        "\n",
        "print(\"MSE(t, y1)\", MSE(t, y1))\n",
        "print(\"MSE(t, y2)\", MSE(t, y2))\n",
        "\n",
        "print(\"tf_MSE(t, y1)\", tf_MSE(t, y1))\n",
        "print(\"tf_MSE(t, y2)\", tf_MSE(t, y2))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eLYfkA2FTaqE"
      },
      "source": [
        "# 3.2 automatic differentiation\n",
        "\n",
        "* 자동으로 미분하기 위해 forward pass 동안 어떤 연산이 어떤 순서로 발생하는지 기억해야 함\n",
        "\n",
        "* 그런 다음 backward pass 중에 gradient 를 계산하기 위해 이 연산 목록을 역순으로 순회함\n",
        "\n",
        "* 이를 위해 tf.GradientTape 을 사용함\n",
        "\n",
        "[자동미분](https://www.tensorflow.org/guide/autodiff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4-z64FdXnAg"
      },
      "outputs": [],
      "source": [
        "x = tf.Variable(2.0) # tf.Variable(2.0, trainable=True)\n",
        "y = tf.Variable(3.0) # tf.Variable(3.0, trainable=True)\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
        "# Record operations for automatic differentiation.\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    z = x**2 + y**2\n",
        "\n",
        "# dx = 2x -> 4\n",
        "# dy = 2y -> 6\n",
        "dx, dy = tape.gradient(z, [x, y])\n",
        "print('dx=', dx.numpy())\n",
        "print('dy=', dy.numpy())\n",
        "\n",
        "x = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x**2\n",
        "\n",
        "# dy = 2x * dx => 6\n",
        "dy_dx = tape.gradient(y, x)\n",
        "print('dy_dx.numpy()', dy_dx.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaDND0z-U-JM"
      },
      "outputs": [],
      "source": [
        "# multiple variables\n",
        "W = tf.Variable(tf.random.normal((3, 2)), name='w')\n",
        "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
        "x = [[1., 2., 3.]]\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  # Since python >= 3.5 the @ operator is supported (see PEP 465).\n",
        "  # In TensorFlow, it simply calls the tf.matmul() function\n",
        "  y = x @ W + b\n",
        "  loss = tf.reduce_mean(y**2)\n",
        "\n",
        "[dl_dW, dl_db] = tape.gradient(loss, [W, b])\n",
        "\n",
        "print('W', W)\n",
        "print('b', b)\n",
        "print('dl_dw', dl_dW)\n",
        "print('dl_db', dl_db)\n",
        "\n",
        "my_vars = {\n",
        "    'W': W,\n",
        "    'b': b\n",
        "}\n",
        "\n",
        "grad = tape.gradient(loss, my_vars)\n",
        "print('grad[W]', grad['W'])\n",
        "print('grad[b]', grad['b'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4FSLtAhk6BRE"
      },
      "source": [
        "# 3.3 MSE with SGD\n",
        "\n",
        "gradient descent 는 계산량이 많아 데이터가 많으면 느리다 (보통 GPU 메모리가 부족함 ㅠㅠ)\n",
        "\n",
        "* f 의 도함수를 구한 후 관찰값의 개수만큼 모두 계산함 (예 1,000개의 센서 데이터)\n",
        "* $x_{1}, x_{2}, x_{3}...$ 등으로 n 개의 features 로 f 값을 계산 시 n개 각각 계산해야 함\n",
        "* epochs이 100 번이라면, 1000 * n * 100 미분\n",
        "\n",
        "반면에, Stochastic Gradient Descent 는\n",
        "\n",
        "* **실제 기울기(전체 데이터 세트에서 계산)를 추정치(데이터 세트에서 임의로 하나 선택하여 계산)로 대체하여 근사함**\n",
        "\n",
        "* 고차원 최적화 문제에서 계산 부담을 줄여줌\n",
        "\n",
        "* but, zig-zag 패턴 및 수렴 속도가 엄청(?) 느려질 수 있고, 하나 고른 데이터가 outlier 일 경우에 영향을 많이 받을 수 있음\n",
        "\n",
        "* mini-batch 전략으로 보완함\n",
        "\n",
        "[확률적 경사 하강법](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
        "\n",
        "<br>\n",
        "\n",
        "| Stochastic | Batch |\n",
        "| ----------- | ----------- |\n",
        "| <img src=\"https://www.mltut.com/wp-content/uploads/2020/04/Untitled-document-5.png\"> | <img src=\"https://www.mltut.com/wp-content/uploads/2020/04/Untitled-document-4.png\"> |\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "<img src=\"https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/48_blog_image_2.png\">\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kbP1fkMFrcjwBO3NVP9OfQ.png\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "id": "V_eMWI0zGAyq",
        "outputId": "ae8c5ab6-c659-497c-cc00-7df5c4acd09a"
      },
      "outputs": [],
      "source": [
        "#Mini Batch Gradient Descent\n",
        "def f(x, w, b):\n",
        "  return w*x + b\n",
        "\n",
        "def MSE(y, t):\n",
        "  return tf.reduce_mean((y-t)**2)\n",
        "\n",
        "def w_grad_MSE(y, t, w, b):\n",
        "  return 2*tf.reduce_mean((y-t_batch)*x_batch)\n",
        "\n",
        "def b_grad_MSE(y, t, w, b):\n",
        "  return 2*tf.reduce_mean((y-t_batch))\n",
        "\n",
        "x = tf.range(-10,10, dtype=tf.float32)\n",
        "t = tf.range(-10,10, dtype=tf.float32)\n",
        "\n",
        "print('x', x)\n",
        "print('t', t)\n",
        "\n",
        "w = tf.Variable(tf.random.normal([]))\n",
        "b = tf.Variable(tf.random.normal([]))\n",
        "\n",
        "#w = tf.Variable(0.5)\n",
        "#b = tf.Variable(0.0)\n",
        "\n",
        "print('w', w)\n",
        "print('b', b)\n",
        "\n",
        "lr = 0.01  # 0.01, learning rate\n",
        "loss_list = []\n",
        "\n",
        "train_size = tf.size(t) # 12\n",
        "batch_size = 4\n",
        "\n",
        "# mini batch SGD\n",
        "K = train_size // batch_size # 3\n",
        "\n",
        "for epoch in range(100):\n",
        "    loss = 0\n",
        "    for step in range(K):\n",
        "        mask = np.random.choice(train_size, batch_size)\n",
        "        #print(mask)\n",
        "        # https://www.tensorflow.org/api_docs/python/tf/gather\n",
        "        # Gather slices from params axis axis according to indices.\n",
        "        x_batch = tf.gather(x, indices=mask)\n",
        "        t_batch = tf.gather(t, indices=mask)\n",
        "\n",
        "        y_batch = f(x_batch, w, b) #w*x_batch + b\n",
        "        loss = MSE(y_batch, t_batch) # calculate MSE\n",
        "        dw = w_grad_MSE(y_batch, t_batch, w, b)\n",
        "        db = b_grad_MSE(y_batch, t_batch, w, b)\n",
        "        #dw = 2*tf.reduce_mean((y-t_batch)*x_batch) # gradients\n",
        "        #db = 2*tf.reduce_mean((y-t_batch))\n",
        "        #print('dw', dw)\n",
        "        #print('db', db)\n",
        "        w.assign_sub(lr*dw)\n",
        "        b.assign_sub(lr*db)\n",
        "\n",
        "    loss /= tf.cast(K, dtype=tf.float32)   # average loss\n",
        "    loss_list.append(loss.numpy())\n",
        "    if not epoch%10:\n",
        "        print(\"epoch={}: w={:>8.4f}. b={:>8.4f}, loss={:>8.4f}\".format(epoch, w.numpy(), b.numpy(), loss.numpy()))\n",
        "\n",
        "print(\"w={:>8.4f}. b={:>8.4f}, loss={:>8.4f}\".format(w.numpy(), b.numpy(), loss.numpy()))\n",
        "\n",
        "plt.plot(loss_list)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "6QEEtOLa3Gom",
        "outputId": "f8714f5d-915b-4d8e-d3c6-3ea9f52dd4d9"
      },
      "outputs": [],
      "source": [
        "#Mini Batch Gradient Descent\n",
        "def f(x, w, b):\n",
        "  return w*x + b\n",
        "\n",
        "def MSE(y, t):\n",
        "  return tf.reduce_mean((y-t)**2)\n",
        "\n",
        "x = tf.range(-10, 10, dtype=tf.float32)\n",
        "t = tf.range(-10, 10, dtype=tf.float32)\n",
        "\n",
        "print('x', x)\n",
        "print('t', t)\n",
        "\n",
        "w = tf.Variable(tf.random.normal([]))\n",
        "b = tf.Variable(tf.random.normal([]))\n",
        "\n",
        "#w = tf.Variable(0.5)\n",
        "#b = tf.Variable(0.0)\n",
        "\n",
        "print('w', w)\n",
        "print('b', b)\n",
        "\n",
        "lr = 0.01  # 0.01, learning rate\n",
        "loss_list = []\n",
        "\n",
        "train_size = tf.size(t) # 12\n",
        "batch_size = 4\n",
        "\n",
        "# mini batch SGD\n",
        "K = train_size // batch_size # 3\n",
        "\n",
        "for epoch in range(100):\n",
        "    loss = 0\n",
        "    for step in range(K):\n",
        "        mask = np.random.choice(train_size, batch_size)\n",
        "        #print(mask)\n",
        "        # https://www.tensorflow.org/api_docs/python/tf/gather\n",
        "        # Gather slices from params axis axis according to indices.\n",
        "        x_batch = tf.gather(x, indices=mask)\n",
        "        t_batch = tf.gather(t, indices=mask)\n",
        "        with tf.GradientTape() as tape:\n",
        "          y_batch = f(x_batch, w, b) #w*x_batch + b\n",
        "          loss = MSE(y_batch, t_batch) # calculate MSE & loss_mean\n",
        "\n",
        "        [dw, db] = tape.gradient(loss, [w,b])\n",
        "        # dW = 2*tf.reduce_mean((y-t_batch)*x_batch) # gradients\n",
        "        # dB = 2*tf.reduce_mean((y-t_batch))\n",
        "        #print('dW', dW)\n",
        "        #print('dB', dB)\n",
        "        w.assign_sub(lr*dw)\n",
        "        b.assign_sub(lr*db)\n",
        "\n",
        "    loss /= tf.cast(K, dtype=tf.float32)   # average loss\n",
        "    loss_list.append(loss.numpy())\n",
        "    if not epoch%10:\n",
        "        print(\"epoch={}: w={:>8.4f}. b={:>8.4f}, loss={:>8.4f}\".format(epoch, w.numpy(), b.numpy(), loss.numpy()))\n",
        "\n",
        "print(\"w={:>8.4f}. b={:>8.4f}, loss={:>8.4f}\".format(w.numpy(), b.numpy(), loss.numpy()))\n",
        "\n",
        "plt.plot(loss_list)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qif2NTMDfnQw"
      },
      "source": [
        "## 3.3.1 tf.keras optimizers\n",
        "\n",
        "### SGD: Gradient descent (with momentum) optimizer.\n",
        "\n",
        "$$\\vec{v_{t}} = \\rho\\cdot\\vec{v_{t-1}} - lr\\cdot\\vec{dw}$$\n",
        "$$w = w + v_{t}$$\n",
        "\n",
        "```python\n",
        "tf.keras.optimizers.SGD(\n",
        "    learning_rate=0.01,  \n",
        "    momentum=0.0,        \n",
        "    ...\n",
        ")\n",
        "```\n",
        "### Adagrad\n",
        "\n",
        "learning rate 에 지금까지 계산한 gradient 제곱값의 누적합(G)를 나눠서 갱신함\n",
        "\n",
        "기울기가 급격하게 변화할 때 lr 값이 작아지고, 반대일 때 커진다.\n",
        "\n",
        "학습을 많이할 수록 learning rate가 점점 작아짐\n",
        "\n",
        "$$w = w - lr\\cdot\\frac{1}{\\sqrt{G+\\epsilon}}\\cdot dw$$\n",
        "\n",
        "```python\n",
        "tf.keras.optimizers.Adagrad(\n",
        "    learning_rate=0.001,\n",
        "    epsilon=1e-07,\n",
        "    ...\n",
        ")\n",
        "```\n",
        "\n",
        "### RMSprop\n",
        "\n",
        "Adagrad의 G를 구할 때, 직전 단계의 G 누적합에 $\\rho$ 값으로 반영 비율을 조절함\n",
        "\n",
        "$$G_{t}\\prime = \\rho \\cdot G_{t-1} + (1-\\rho)\\cdot (dw_{t})^2$$\n",
        "\n",
        "$$w = w - lr\\cdot\\frac{1}{\\sqrt{G\\prime+\\epsilon}}\\cdot dw$$\n",
        "\n",
        "```python\n",
        "tf.keras.optimizers.experimental.RMSprop(\n",
        "    learning_rate=0.001,\n",
        "    rho=0.9,\n",
        "    ...\n",
        ")\n",
        "```\n",
        "\n",
        "### Adam\n",
        "\n",
        "RMSprop 방법에 momentum 을 추가한 방법임\n",
        "\n",
        "현재 가장 널리 사용되고 있는 최적화 알고리즘\n",
        "\n",
        "```python\n",
        "tf.keras.optimizers.Adam(\n",
        "    learning_rate=0.001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    ...\n",
        ")\n",
        "\n",
        "```\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/11681225/50016682-39742a80-000d-11e9-81da-ab0406610b9c.gif\">\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/11681225/49325458-fc785480-f585-11e8-8d2a-9012d6024c6e.gif\">\n",
        "\n",
        "[멋진 설명(33-50p)](https://www.slideshare.net/yongho/ss-79607172) - Kakao 하용호님\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/45377884/91630397-18838100-ea0c-11ea-8f90-515ef74599f1.png\">\n",
        "\n",
        "\n",
        "\n",
        "[Animation 참조](https://github.com/ilguyi/optimizers.numpy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "1hj5EPe1gLe1",
        "outputId": "c7d916da-6e25-4ad5-e920-24b843e60ffd"
      },
      "outputs": [],
      "source": [
        "x = tf.Variable(2.0)\n",
        "y = tf.Variable(3.0)\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental/SGD\n",
        "# Gradient descent (with momentum) optimizer.\n",
        "opt = tf.keras.optimizers.SGD(learning_rate=0.1) # learning_rate=0.001\n",
        "#opt = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
        "#opt = tf.keras.optimizers.RMSprop(learning_rate=0.1)\n",
        "\n",
        "loss_list = [ ]\n",
        "for epoch in range(100):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = x**2 + y**2\n",
        "  loss_list.append(loss.numpy())\n",
        "  #if loss < 0.001: break\n",
        "  dx, dy = tape.gradient(loss, [x, y])\n",
        "  grads_and_vars = zip([dx, dy], [x, y])\n",
        "  opt.apply_gradients(grads_and_vars)\n",
        "\n",
        "  if not epoch%10:\n",
        "    print(\"epoch={}: w={:>8.4f}. b={:>8.4f}, loss={:>8.4f}\".format(epoch, x.numpy(), y.numpy(), loss.numpy()))\n",
        "\n",
        "\n",
        "print(\"w={:>8.4f}. b={:>8.4f}, loss={:>8.4f}\".format(x.numpy(), y.numpy(), loss.numpy()))\n",
        "\n",
        "plt.plot(loss_list)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3Y4SMgOa4YC"
      },
      "outputs": [],
      "source": [
        "# def MSE(y, t):\n",
        "#     return tf.reduce_mean((y-t)**2)\n",
        "\n",
        "tf_MSE = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "\n",
        "x = tf.range(20, dtype=tf.float32)\n",
        "t = tf.range(20, dtype=tf.float32)\n",
        "\n",
        "w = tf.Variable(0.5)\n",
        "b = tf.Variable(0.0)\n",
        "\n",
        "lr = 0.001  # 0.01, learning rate\n",
        "loss_list = []\n",
        "\n",
        "train_size = tf.size(t) # 12\n",
        "batch_size = 4\n",
        "\n",
        "# mini batch SGD\n",
        "K = train_size // batch_size # 3\n",
        "\n",
        "for epoch in range(100):\n",
        "    loss = 0\n",
        "    for step in range(K):\n",
        "        mask = np.random.choice(train_size, batch_size)\n",
        "        #print(mask)\n",
        "        x_batch = tf.gather(x, indices=mask)\n",
        "        t_batch = tf.gather(t, indices=mask)\n",
        "        with tf.GradientTape() as tape:\n",
        "          y = w*x_batch + b                          # calculate the output\n",
        "          loss += tf_MSE(y, t_batch) # calculate MSE\n",
        "        optimizer.minimize(loss, [w, b], tape=tape)\n",
        "        #dW = 2*tf.reduce_mean((y-t_batch)*x_batch) # gradients\n",
        "        #dB = 2*tf.reduce_mean((y-t_batch))\n",
        "        #print('dW', dW)\n",
        "        #print('dB', dB)\n",
        "        #w.assign_sub(lr*dW)\n",
        "        #b.assign_sub(lr*dB)\n",
        "\n",
        "    loss /= tf.cast(K, dtype=tf.float32)   # average loss\n",
        "    loss_list.append(loss.numpy())\n",
        "    if not epoch%10:\n",
        "        print(\"epoch={}: w={:>8.4f}. b={:>8.4f}, loss={:>8.4f}\".format(epoch, w.numpy(), b.numpy(), loss.numpy()))\n",
        "\n",
        "print(\"w={:>8.4f}. b={:>8.4f}, loss={:>8.4f}\".format(w.numpy(), b.numpy(), loss.numpy()))\n",
        "\n",
        "plt.plot(loss_list)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OynmCv0DZiOt"
      },
      "source": [
        "# 3.4 simple linear regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "973J3NpjJ9rS",
        "outputId": "6883e9a7-13e2-4377-ef13-0e1cc1ca70ce"
      },
      "outputs": [],
      "source": [
        "# https://www.tensorflow.org/tutorials/keras/regression\n",
        "# Predict fuel efficiency with the Auto MPG dataset\n",
        "\n",
        "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
        "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
        "                'Acceleration', 'Model Year', 'Origin']\n",
        "\n",
        "raw_dataset = pd.read_csv(url, names=column_names,\n",
        "                          na_values='?', comment='\\t',\n",
        "                          sep=' ', skipinitialspace=True)\n",
        "MPG_dataset = raw_dataset.copy()\n",
        "MPG_dataset.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv_ktwsZPYX-",
        "outputId": "09b432f6-18aa-434e-add9-5689238dcf8a"
      },
      "outputs": [],
      "source": [
        "print(MPG_dataset.isna().sum())\n",
        "MPG_dataset = MPG_dataset.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "KmaVwB5pKO5t",
        "outputId": "6af1b495-559c-412c-d3ef-10f5c09f9156"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "train_dataset = MPG_dataset.sample(frac=0.8, random_state=0)\n",
        "test_dataset = MPG_dataset.drop(train_dataset.index)\n",
        "sns.pairplot(train_dataset[['MPG', 'Cylinders', 'Displacement', 'Weight']], diag_kind='kde')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "lyqdT_RbKjvt",
        "outputId": "e2550ec1-56a7-4e29-9a4f-b42de12f7795"
      },
      "outputs": [],
      "source": [
        "train_dataset.corr() # miles per gallon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_eeUGg0cgkR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lDIfwnHLTee"
      },
      "outputs": [],
      "source": [
        "def load_MPG_train_dataset(columns):\n",
        "  x = train_dataset[columns].to_numpy()\n",
        "  y = train_dataset[['MPG']].to_numpy()\n",
        "  return (x, y)\n",
        "\n",
        "def load_MPG_test_dataset(columns):\n",
        "  x = test_dataset[columns].to_numpy()\n",
        "  y = test_dataset[['MPG']].to_numpy()\n",
        "  return (x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "jxAPtkfUOZcc",
        "outputId": "08930bc8-3618-4cc7-872f-8df980bf4c35"
      },
      "outputs": [],
      "source": [
        "# gradient descent\n",
        "x, y = load_MPG_train_dataset(['Horsepower'])\n",
        "\n",
        "w = tf.Variable(tf.random.normal([]))\n",
        "b = tf.Variable(tf.random.normal([]))\n",
        "\n",
        "loss_list = []\n",
        "\n",
        "tf_MSE = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "\n",
        "for epoch in range(100):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_hat = w * x + b\n",
        "    loss = tf_MSE(y_hat, y)\n",
        "\n",
        "  dw, db = tape.gradient(loss, [w, b])\n",
        "  opt.apply_gradients([[dw, w], [db, b]])\n",
        "  loss_list.append(loss.numpy())\n",
        "\n",
        "  if not epoch%10:\n",
        "    print(\"epoch={}: w={:>8.4f}. b={:>8.4f}, loss={:>8.4f}\".format(epoch, w.numpy(), b.numpy(), loss.numpy()))\n",
        "\n",
        "print(\"w={:>8.4f}. b={:>8.4f}, loss={:>8.4f}\".format(w.numpy(), b.numpy(), loss.numpy()))\n",
        "\n",
        "plt.plot(loss_list)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "-PU_1IUxSpvP",
        "outputId": "c0b50249-8c4a-493b-863d-6db115864817"
      },
      "outputs": [],
      "source": [
        "# mini-batch gradient descent\n",
        "x, y = load_MPG_train_dataset(['Horsepower'])\n",
        "batch_size = 32\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "w = tf.Variable(tf.random.normal([]))\n",
        "b = tf.Variable(tf.random.normal([]))\n",
        "\n",
        "loss_list = []\n",
        "\n",
        "tf_MSE = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "for epoch in range(100):\n",
        "  dataset_train = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "  for step, (x_batch, y_batch) in enumerate(dataset_train):\n",
        "    #print('step', step)\n",
        "    #print('x_batch', x_batch.shape)\n",
        "    #print('y_batch', y_batch.shape)\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_hat = w * x + b\n",
        "      loss = tf_MSE(y_hat, y)\n",
        "\n",
        "    dw, db = tape.gradient(loss, [w, b]) #미분해줘\n",
        "    opt.apply_gradients([[dw, w], [db, b]])\n",
        "    loss/=batch_size\n",
        "    loss_list.append(loss.numpy())\n",
        "\n",
        "  if not epoch%10:\n",
        "    print(\"epoch={}: w={:>8.4f}. b={:>8.4f}, loss={:>8.4f}\".format(epoch, w.numpy(), b.numpy(), loss.numpy()))\n",
        "\n",
        "print(\"w={:>8.4f}. b={:>8.4f}, loss={:>8.4f}\".format(w.numpy(), b.numpy(), loss.numpy()))\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "axs[0].plot(loss_list)\n",
        "axs[0].set(xlabel=\"epochs\", ylabel=\"loss\")\n",
        "\n",
        "axs[1].scatter(x, y)\n",
        "axs[1].plot(x, w*x+b, color='g')\n",
        "axs[1].set(xlabel=\"Horsepower\", ylabel=\"MPG\")\n",
        "\n",
        "x, y = load_MPG_test_dataset(['Horsepower'])\n",
        "axs[2].scatter(x, y)\n",
        "axs[2].plot(x, w*x+b, color='r')\n",
        "axs[2].set(xlabel=\"Horsepower\", ylabel=\"MPG\")\n",
        "print(\"loss={:>8.4f}\".format(tf_MSE(w*x+b, y)))\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "i7GWC3OAZn66"
      },
      "source": [
        "# 3.5 multiple linear regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "2WEUCfGFc_51",
        "outputId": "91cb1386-182e-469c-8e71-469c5487e0a5"
      },
      "outputs": [],
      "source": [
        "# mini-batch gradient descent\n",
        "x, y = load_MPG_train_dataset(column_names[1:])\n",
        "batch_size = 32\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "w = tf.Variable(tf.random.normal(shape=(x.shape[1],1)))\n",
        "b = tf.Variable(tf.random.normal([]))\n",
        "\n",
        "loss_list = []\n",
        "\n",
        "tf_MSE = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "for epoch in range(100):\n",
        "  dataset_train = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "  for step, (x_batch, y_batch) in enumerate(dataset_train):\n",
        "    #print('step', step)\n",
        "    #print('x_batch', x_batch.shape)\n",
        "    #print('y_batch', y_batch.shape)\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_hat = x @ w + b\n",
        "      loss = tf_MSE(y_hat, y)\n",
        "\n",
        "    dw, db = tape.gradient(loss, [w, b])\n",
        "    opt.apply_gradients([[dw, w], [db, b]])\n",
        "    loss/=batch_size\n",
        "    loss_list.append(loss.numpy())\n",
        "\n",
        "  if not epoch%10:\n",
        "    print(\"epoch={}: loss={:>8.4f}\".format(epoch, loss.numpy()))\n",
        "\n",
        "print(\"loss={:>8.4f}\".format(loss.numpy()))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15, 5))\n",
        "ax.plot(loss_list)\n",
        "ax.set(xlabel=\"epochs\", ylabel=\"loss\")\n",
        "\n",
        "#axs[1].scatter(x, y)\n",
        "#axs[1].plot(x, w*x+b, color='g')\n",
        "#axs[1].set(xlabel=\"Horsepower\", ylabel=\"MPG\")\n",
        "\n",
        "x, y = load_MPG_test_dataset(column_names[1:])\n",
        "#axs[2].scatter(x, y)\n",
        "#axs[2].plot(x, w*x+b, color='r')\n",
        "#axs[2].set(xlabel=\"Horsepower\", ylabel=\"MPG\")\n",
        "print(\"loss={:>8.4f}\".format(tf.reduce_mean(tf_MSE(x@w+b, y))))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvg5fPQK2Fo6"
      },
      "source": [
        "# 과제 002\n",
        "\n",
        "주어진 보스톤 주택 가격 데이터 셋을 이용하여 주택 가격을 예측하는 multiple variables regression 을 수행하는 프로그램을 직접 작성하시오.\n",
        "\n",
        "단, 아래 조건을 만족하여 구현하시오.\n",
        "\n",
        "* loss 함수: tensorflow 의 MSE\n",
        "* gradient 계산: tensorflow의 GradientTape()\n",
        "* optimizer: tensorflow의 Adam\n",
        "\n",
        "**제출**\n",
        "\n",
        "구현한 코드가 담긴 colab 파일을 PLATO \"텐서플로 과제 제출 01\"에 제출하세요. (23/7/6까지)\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/datasets/boston_housing/load_data\n",
        "# features 설명 (http://lib.stat.cmu.edu/datasets/boston)\n",
        "# 보스톤 주택 가격(MEDV) 예측 (1인당 범죄율, 주택당 평균 방 개수, 학생대 교사 비율 등의 features 이용함)\n",
        "# MEDV(주택 가격 중앙값, 단위: $1,000)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data(\n",
        "    path='boston_housing.npz', test_split=0.2, seed=113\n",
        ")\n",
        "print('x_train.shape', 'y_train.shape', x_train.shape, y_train.shape)\n",
        "\n",
        "all_train_data = np.hstack((x_train, y_train.reshape((-1, 1))))\n",
        "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
        "df = pd.DataFrame(all_train_data, columns=column_names)\n",
        "df.head()\n",
        "sns.pairplot(df[['CRIM', 'ZN', 'INDUS', 'TAX', 'MEDV']], diag_kind='kde')\n",
        "```\n",
        "\n",
        "* 힌트:\n",
        "https://dschloe.github.io/python/tensorflow2.0/ch4_4_boston_housing_deeplearning/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
